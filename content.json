{"pages":[],"posts":[{"title":"HQL (Hive SQL)명령어 모음","text":"Data Definition Language (DDL) 다음은 HIVE 데이터베이스의 핵심 DDL이다. CREATE 1234create database hivedatabase; ---데이터베이스 생성create database if exists hivedatabase ---데이터베이스 존재유무 확인 후 생성comment 'Hive database'; ---데이터베이스 설명 추가 SHOW 12show databases; ---모든 데이터베이스를 보여준다.show databases like 'hive*'; ---문자열에 해당하는 데이터베이스 조회 DESCRIBE 1describe database hivedatabase; --- 해당 데이터베이스를 와일드카드로 설명한다. USE 1use hivedb; --- hivedb 라는 데이터베이스를 사용한다. ALTERrename to, set tblproperties, set dbproperties, set serdepropertiesset fileformat, set location, 12345678910111213141516171819202122232425262728293031데이터베이스 관리alter database hivedbset dbproperties ('edited-by' = 'khk'); --- 데이터 베이스 속성을 변경할 수 있다.alter database hivedbset owner user khk; --- 데이터베이스 권한을 변경한다.alter database hivedbrename to hadoopdb; --- 테이블 이름을 변경한다.--------------------------------------------------------------------------테이블 관리alter table hivedbset serdeproperties ('field.delim' = '$'); --- 테이블 구분자를 변경한다.alter table hivedbset fileformat rcfile; --- 테이블 파일 포맷을 변경한다.alter table hivedb CONCATENATE; --- 작은 파일을 큰 파일로 병합alter table hivedb set fileformat TEXTFILE; --- 일반 파일 포맷으로 변환alter table hivedb ENABLE NO_DROP; --- 테이블 삭제 방지alter table hivedb DSIABLE NO_DROP;alter table hivedb ENABLE OFFLINE; --- 쿼리로부터 테이블의 데이터 보호alter table hivedb DISABLE OFFLINE;--------------------------------------------------------------------------칼럼의 타입과 순서 변경alter table hivedb change user_name changed_name string FIRST; --- 칼럼명 user_name을 changed_name으로 변경하고 string 타입으로 첫번째 자리에 배치한다.alter table hivedb add columns (work string); --- 칼럼 추가 컬럼명:work 타입:string DROP 12drop database if exists hivedb; --- hivedb 존재유무 확인 후 데이터베이스를 삭재한다.drop database if exists hivedb cascade; -- 영구 삭제 TRUNCATE 1truncate table hivedb; --- 테이블의 모든 row들을 삭제한다. 즉, 테이블을 비운다. 테이블 복사 생성 1create table hivedb as select * from sample_table; --- 'sample_table'테이블의 값을 복사해 생성한다. unsplash-logoDamien TUPINIER","link":"/2018/01/27/HQLcommands/"},{"title":"CUDA + cuDNN + Tensorflow installation guide (Ubuntu)","text":"My Environment OS : Ubuntu 16.04 GPU : GT 940MX (2G) 1. Blacklist 설정Ubuntu 16.04버전 설치시 기본으로 탑재된 그래픽카드 드라이버가 CUDA설치에 문제를 일으킬 수 있으므로 블랙리스트 설정을 통해 실행을 막습니다. 편집기로 아래 파일을 수정합니다. 1sudo vi /etc/modprobe.d/blacklist-nouveau.conf 아래 내용을 입력하고 저장합니다. 12345blacklist nouveaublacklist lbm-nouveauoptions nouveau modeset=0alias nouveau offalias lbm-nouveau off 아래 명령을 통해 커널 정보를 업데이트한 후 재부팅합니다. 1sudo update-initramfs -u 1sudo reboot 2. NVIDIA 드라이버 설치NVIDIA 홈페이지에 접속하여 드라이버를 로컬에 다운로드 받습니다. 사이트 URL : http://www.nvidia.co.kr/Download/index.aspx?lang=kr NVIDIA 드라이버 설치는 터미널 환경에서 진행하기를 권장합니다. 아래 키를 입력하여 터미널환경으로 접속합니다. Ctrl + Alt + f1 or ~f9 아래 명령어를 입력하여 우분투 윈도우 그래픽 서비스를 중단시킵니다. 1sudo service lightdm stop 사전에 설치된 NVIDIA 그래픽 드라이버 제거합니다. 이 부분은 넘어가셔도 상관없으나 아래 명령을 통해 사전에 설치된 그래픽 드라이버를 제거할 수 있습니다. 12sudo /usr/bin/nvidia-uninstallsudo apt-get remove --purge nvidia* 다운로드된 NVIDIA 그래픽 드라이버 설치파일 경로로 이동하여 다운로드한 파일을 실행시킵니다. 1234cd ~/Downloadschmod a+x NVIDA-Linux ~.runsudo sh NVIDIA~.run 설치가 완료되면 nvidia-smi 명령어를 실행해봅니다. nvidia-smi는 그래픽 카드 디바이스 모니터링 기능을 제공합니다. 1nvidia-smi 2.1 apt 명령을 통한 설치드라이버 설치파일을 다운로드하여 실행시키는 방법 외에, NVIDIA의 레포지토리를 등록하여 apt 명령으로 설치가 가능합니다. 아래 명령을 통해 그래픽 드라이버 레포지토리를 등록한 후 설치를 진행할 수 있습니다. 123sudo add-apt-repository ppa:graphics-drivers/ppasudo apt updatesudo apt install nvidia-390 3. CUDA 9.0 설치CUDA 설치를 진행하기 전에 사용할 딥러닝 프레임워크(keras, tensorflow, chainer 등)버전과 CUDA버전이 호환되는지 필수적으로 확인해야합니다. 현재 글을 쓰는 시점에서(2018.6.4) 최신 CUDA 버전은 9.2 이지만 2018년 4월 말에 공개된 Tensorflow 1.8 버전은 CUDA 9.0 을 지원하기 때문에 해당 버전 설치를 진행하겠습니다. NVIDIA의 CUDA 다운로드 페이지에 접속하여 설치파일을 다운로드 받습니다. CUDA 9.0 다운로드 URL : https://developer.nvidia.com/cuda-90-download-archive 아래 그림과 같이 Ubuntu 16.04 버전에 해당하는 설치파일을 다운로드 받습니다. CUDA도 NVIDIA 그래픽 드라이버 설치와 동일하게 윈도우 그래픽 을 중단한 상태로 진행해야합니다. Ctrl + Alt + f1 or ~f9 1sudo service lightdm stop 다운로드한 CUDA 설치 파일 경로로 이동하여 실행시킵니다. 1234cd ~/Downloadschmod a+x cuda_9.0~.runsudo sh cuda_9.0~.run 설명문이 나오면 Enter 혹은 space 를 입력하여 끝까지 읽고 (Ctrl + c 로 스킵 가능) 다음처럼 진행합니다. “”Do you accept the previously read EULA? accept/decline/quit: accept Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81? ((y)es /(n)o/(q)uit) : y Do you want to install the OpenGL libraries? ((y)es /(n)o/(q)uit) : n Do you want to run nvidia-xconfig?This will update the system X configuration file so that the NVIDIA X driveris used. The pre-existing X configuration file will be backed up.This option should not be used on systems that require a customX configuration, such as systems with multiple GPU vendors. ((y)es /(n)o/(q)uit) : n Install the CUDA 9.0 Toolkit? ((y)es /(n)o/(q)uit) : y Enter Toolkit Location[ default is /usr/local/cuda-9.0 ] : Enter Do you want to install a symbolic link at /usr/local/cuda? ((y)es /(n)o/(q)uit) : y Install the CUDA 9.0 Samples?(y)es/(n)o/(q)uit: y Enter CUDA Samples Location[ default is /home/khk ] : Enter 위 까지 진행하시면 설치가 시작되며 다음으로 cuDNN 설치를 진행하시길 바랍니다. 4. cuDNN 설치cuDNN은 GPU 가속을 이용할 수 있도록 NVIDIA에서 제공하는 라이브러리입니다. NVIDIA의 cuDNN 다운로드 링크를 접속하여 간단한 멤버십 가입을 통해 다운로드를 진행합니다. 다운로드 URL : https://developer.nvidia.com/rdp/cudnn-download CUDA 9.0 버전의 제일 상단의 cuDNN v7.1.4 Library for Linux 을 선택하여 다운로드합니다. (tgz 파일) 다운로드 받은 파일 경로로 이동하여 압축을 해제합니다. 123cd Downloadstar xvf cudnn-9.0-linux-x64-v7.1.tgz 압축을 해제하면 cuda 폴더가 생성되는데, 내부 파일들을 다음과 같이 복사합니다. 123sudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+x /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 모든 과정이 끝나면 메인으로 이동하여 .bashrc 파일에 CUDA의 PATH를 등록합니다. 1234sudo vi ~/.bashrcexport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 내용을 저장하고 닫습니다. bashrc의 변경사항을 바로 반영하도록 다음 명령을 실행합니다. 1source ~/.bashrc 5. Tensorflow 환경 구축5.1 Python 설치아래와 같이 파이썬 3, 2 버전 별로 Anaconda 배포판 설치 스크립트 다운로드 받습니다. Python3.6.x 버전 (x64) 1wget https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh 1chmod u+x Anaconda3-5.1.0-Linux-x86_64.sh 1bash Anaconda3-5.1.0-Linux-x86_64.sh Python 2.7.x (x64) 1wget https://repo.continuum.io/archive/Anaconda2-5.1.0-Linux-x86_64.sh 1chmod u+x Anaconda2-5.1.0-Linux-x86_64.sh 1bash Anaconda2-5.1.0-Linux-x86_64.sh 본 가이드에서는 /opt 에 anaconda3 설치했다는 가정하에 진행하겠습니다. vi 편집기로 .bashrc 파일에 아래와 같이 내용을 입력하고 저장합니다. 12345vi ~/.bashrcexport PATH=/opt/anaconda3/bin:$PATHsource ~/.bashrc 5.3 Jupyter notebook주피터 노트북을 실행하기 앞서 홈디렉토리로 이동하여 파이썬을 실행합니다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273python```주피터 노트북의 비밀번호를 생성하기 위해 아래와 같이 코드를 실행시킵니다.output으로 생성된 `sha1:xxx` 값을 복사해둡시다.```pythonln [1]: from notebook.auth import passwdln [2]: passwd()Enter password: [비밀번호입력]Verity password: [비밀번호입력]Out[2]: [sha1: xxx 를 복사 후 별도로 메모 or 복사할 것]ln [3]: exit()```아래 명령어를 통해 홈디렉토리에 주피터 노트북의 설정파일을 생성시킵니다. ```bashjupyter notebook --generate-config```설정파일을 편집기로 열어서 아래와 같이 입력시킵니다.```shvi ~/.jupyter/jupyter_notebook_config.py...c.NotebookApp.password = '복사한 sha1:xxx 붙여넣기'...c.NotebookApp.ip = 'IP 입력'...c.Notebook_open_browser = False```편집 내용을 저장시키고 아래 명령어를 통해 주피터 노트북을 실행시킬 수 있습니다.```bashjupyter notebook```웹 브라우저를 실행시키고 주피터 설정파일에 입력한 `ip`와 포트번호 `8888`을 통해 주피터에 접속할 수 있습니다. http://ip:8888###5.2 Tensorflow 설치Tensorflow는 CPU를 이용한 `tensorflow` 버전과 GPU를 이용한 `tensorflow-gpu` 가 따로 존재합니다.우분투 터미널로 돌아와서 아래 명령어를 실행시킵니다.```bashpip install tensorflow-gpu```설치가 완료되면 주피터 접속하여 노트북을 실행시키고 아래 코드를 통해 설치된 그래픽 카드를 확인하실 수 있습니다.```pythonfrom tensorflow.python.client import device_libdevice_lib.list_local_devices()```![gpu](https://drive.google.com/uc?id=1K7t1gvo1XEf5gT_QcywuwIlHnH7qcchu) unsplash-logoFredrick Tendong","link":"/2018/06/14/CUDA_UBUNTU/"},{"title":"Hadoop 설치 및 설정","text":"My Environment Master 1 Slave 1 모든 과정은 root계정으로 진행을 권장합니다. HADOOP 2.7+ 버전의 하둡 파일을 /usr/local 에 다운로드 받는다. (아니면 홈디렉토리에) 1$ tar xvf hadoop-xxx 1$ mv hadoop-xxx hadoop /usr/local 에 압축을 해제했다면 1$ chown -R (유저명):(그룹명) hadoop 하둡은 JAVA를 사용하므로 JAVA 환경변수를 설정한다. 아래 명령어로 자바경로를 잡자. 1$ cd /usr/local/hadoop/etc/hadoop 123$ vi hadoop-env.sh...export JAVA_HOME=/(자바경로) 하둡 환경변수를 등록한다. (편의상) 123$ vi .bashrc … export $HADOOP_HOME = /usr/local/hadoop … export PATH=$PATH:$HADOOP_HOME/bin 하둡은 SSH 통신을 사용하므로 SSH를 설치하고(설치과정 생략) Master 와 Slave 가 공개키를 가지고 있어야 한다. Master PC 홈디렉토리에서 1$ ssh-keygen -t rsa (비밀번호 설정 세번 물어보는데 전부 Enter로 넘긴다.) ~/.ssh/ 안에 id_rsa 와 id_rsa.pub 파일이 생성된다. 1$ cp ./id_rsa.pub ./authorized_keys 1$ chmod 755 ~/.ssh 1$ chmod 644 ~/.ssh/authorized_keys 1$ ssh-add 여기서 에러뜨면 아래 명령 1$ eval $(ssh-agent) 1$ ssh-add Slave PC 홈디렉토리에서 1$ chmod 755 ~/.ssh 로 접근권한을 설정한다. 다시 Master PC 에서 1$ scp ./authorized_keys (ID)@slave1:./.ssh Master 서버의 authorized_keys 파일을 slave1 으로 복사 시킨다. MASTER PC 에서 설정 1$ vi /etc/hosts 편집기로 열어서 IP 와 호스트네임명을 입력한다. $HADOOP_HOME/etc/hadoop 폴더의 core-site.xml mapred-site.xml hdfs-site.xml slaves 파일들의 내용을 수정해야 한다. core-site.xml hdfs-site.xml Datanode 경로는 /opt/hadoop/hdfs/datanode Namenode 경로는 /opt/hadoop/hdfs/namenode 로 정했다. 이 부분은 사용자가 마음대로 지정해도 되나 데이터들이 저장되는 경로기 때문에 용량이 큰 디렉토리로 지정한다. dfs.replication부분의 value값은 노드 갯수만큼으로 지정한다. mapred-site.xml slaves 1$ $HADOOP_HOME/bin/hadoop namenode -format HDFS를 포맷시키는 명령이다. 처음 시작전에 한번 입력해주자 1$ $HADOOP_HOME/sbin/start-all.sh MadpReduce 와 HDFS 데몬을 시작한다. Tip방화벽 22번 포트를 개방시키는게 편하다. 클러스터와의 통신을 ssh로 하기 때문에 ssh 포트를 변경하거나 기본 ssh포트인 22번을 개방시켜야 한다. unsplash-logoHarshil Gudka","link":"/2017/10/20/HadoopInstall/"},{"title":"Windows + Docker 로 Jupyter 환경만들기","text":"앞서 Linux상에서의 Jupyter 서버 설치에 관해 설명드렸는데 이번에는 Windows 상에서 간단하게 Docker를 이용한 서버설치를 설명드리겠습니다. Anaconda3+ 버전을 이용한 설치방법도 있지만Docker만의 컨테이너를 이용한 이미지 관리의 편리함을 느껴보시길 바랍니다. Requirements1. Windows 버전Windows 7+ 이상의 버전 64bit 운영체제가 필요합니다. 2. 가상화 지원CPU 및 메인보드에서 가상화를 지원해야 합니다. 또한 시스템 상에서도 가상화 지원을 enable시켜야 합니다. Docker 이해 위 그림과 같이 Docker는 Docker Engine이라는 일종의 데몬 프로그램으로 Windows Server나 Linux Kernel상의 이미지를 생성해 구동되는 방식입니다. 이는 Container라고 부릅니다. Container라는 것은 운영체제를 구동할 전체 OS 소스를 포함시키지는 않고 Docker Engine에 바로 접목시켜 구동될 수 있는 /bin 나 /lib 특정 프로그램을 실행시킬 패키지만 가지고 있습니다. 즉, 이미 존재하는 Docker라는 리눅스 시스템상에서 특정 프로그램을 실행시킬 실행파일과 그에 필요한 파일을 Container로 묶어 필요할때마다 땟다 붙였다 할 수 있는 겁니다. Container는 다양한 오픈소스 프로그램들을 실행할 수 있도록 사용자들이 만들어 놓은 이미지가 커뮤니티를 통해 공개되어 있습니다. 우리는 그저 입맛에 맞는 것만 받아 설치하면 됩니다. Docker 설치우선 링크 에 접속하여 파일을 Windows용 exe설치 파일을 다운받습니다. (Community 버전) Toolbox 설치다운로드 받은 파일을 실행시키면 Docker toolbox 설치를 위한 화면이 나타납니다. 설치할 위치를 지정시켜주면 선택옵션이 나타나는데 Git이 설치되있다면 체크를 해지해주시고 나머지는 체크된 상태로 진행하면 됩니다. 다음으로는 쭉 Next로 진행하시면 설치가 자동으로 완성됩니다. Jupyter 설치docker 설치가 완료되었다면 이제 Jupyter 컨테이너를 받아서 실행 해보도록 합시다. 윈도우키 + r , cmd 입력으로 윈도우 커맨드창을 관리자권한으로 엽니다.그 후 아래 명령어들을 통해서 docker를 처음 시작해 봅시다. 1docker run --name jupyter -it --rm -p 8888:8888 jupyter/datascience-notebook 위 명령어를 입력하면 설치가 진행됩니다. 이는 https://hub.docker.com/r/jupyter/datascience-notebook/docker 컨테이너를 공유하는 주소로부터 컨테이너를 받아오는 것입니다.사실 jupyter 관련한 컨테이너 종류가 수도없이 많지만 저는 위 컨테이너가 구성이 잘되어있어서 선택하였습니다. 위 링크에서 컨테이너를 시작하는 여러가지 옵션들이 있습니다. 12345678910111213141516docker [option]run : 컨테이너 생성 명령어 입니다.--rm : 기존 컨테이너를 삭제하고 새로운 컨테이너를 만듭니다. 이는 새로운 컨테이너를 생성할때만 입력해줍시다.-p : 컨테이너 내부 포트와 외부 포트를 연결시킵니다. 우리는 이 포트를 이용하여 접속할 수 있습니다. [내부포트]:[외부포트]jupyter/datascience-notebook : Docker hub에서 공유된 주소입니다. 다른 컨테이너를 사용하실려면 이 부분을 수정합니다.--name : 컨테이너의 이름을 지정합니다. 사용자 마음대로 지정하고 이 이름으로 컨테이너를 제어할 수 있습니다.*추가옵션*--NoterbookApp.password='sha1:...'= 초기 비밀번호는 설정합니다. 비밀번호 생성방법은 https://kipang.github.io/blog/JupyterInstall/에서 sha1 키 생성방법을 확인하시길 바랍니다.-d= 백그라운드에서 동작합니다. 즉 컨테이너를 실행시키면 jupyter log들이 쭉 펼쳐지고 커맨드창을 닫으면 jupyter가 종료되는데 이를 모두 백그라운드로 숨기는 것입니다. 주의위 명령어는 처음 컨테이너를 시작하는 명령어로 새로운 컨테이너를 시작하는 것이 아니라면 처음 한번만 실행합시다. 설치가 완료되었다면 브라우저 주소에 http://localhost:8888 를 입력하여 접속해봅시다. 비밀번호는 jovyan입니다. —NotebookApp.password 옵션으로 개인 비밀번호를 설정할 수도 있습니다. Jupyter 실행 및 종료종료Docker를 실행시킨 윈도우 커맨드창을 닫거나 Ctrl+c를 입력하여 종료시킬 수 도 있고 docker stop jupyter 를 통해 컨테이너를 종료시킬 수 있습니다. 실행컨테이너를 재 실행 시키기 위해선 docker start jupyter 를 입력하시면 됩니다. jupyter는 처음 docker run --name jupyter에서 입력한 옵션-—name에서 지정한 이름입니다. unsplash-logochuttersnap","link":"/2018/01/24/JupyterDocker/"},{"title":"Jupyter Server 설치 (Linux)","text":"리눅스상에 python Jupyter Server 설치 및 비밀번호 설정과정입니다. 윈도우 설치와는 다르게 터미널상에서 대화형 설치가 가능합니다. 윈도우 설치 과정은 여기에서 다루겠습니다. Anaconda 다운로드 (2.4버전)1$ wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh Bash 로 설치1$ bash Anaconda2-***.sh 대화형 설치 진행12Enter, Yes 반복합니다.특히 PATH 추가하는 질문에서 꼭 Yes Jupyter 설정 파일 생성1$ jupyter --generate-config 설정파일을 생성하는 명령어 입니다. 위 명령어를 실행하지 않으면 기본설정이 유지됩니다. 아래 명령어로 노트북의 기본 비밀번호를 설정합니다. 12345$ ipythonln [1]: from notebook.auth import passwdln [2]: passwd()Enter password:Verify password: 이떄 생성되는 ‘sha1:xxxxx’ 값을 복사해둡니다. 다시 생성한 설정파일을 열어서 아래와 같이 수정합니다. 123456789$ vi ~/.jupyter/jupyter_notebook_config.py...c.NotebookApp.ip = '192.xxx' //ip를 입력합니다. 외부 접속을 위해성 고정 IP를 입력합니다....c.NotebookApp.password = u’sha1:xxxx’ //복사해 두었던 sha1값을 붙여넣습니다....c.NotebookApp_open_browser = False:wq! 그 외에도 포트번호나 기본 디렉토리의 부분도 변경가능합니다. [ESC]/port 와 같이 입력하여 해당 부분을 수정하시기 바랍니다. Jupyter 실행쉘로 돌아와서 jupyter notebook을 실행합니다. 1$ jupyter notebook 이제 브라우저를 키고 주소창에 입력합니다. https://(IP):8888 8888은 포트번호이고 기본값입니다. 위 설정을 통해 변경가능합니다. unsplash-logoMarkus Spiske","link":"/2017/06/22/JupyterInstall/"},{"title":"파이썬으로 네이버 뉴스 기사 수 크롤링","text":"파이썬 언어를 이용하여 네이버에서 특정 영화와 관련된 뉴스 컨텐츠들의 갯수를 크롤링하는 방법에 알아봅시다. 우선 사용할 패키지들에 대해서 먼저 언급하면 12requestsBeautifulsoup 입니다. 사전에 설치가 된 상태로 진행하시길 바랍니다. 또한 검색 중에 크롬 개발자 도구를 사용하므로 Google Chrome 브라우저를 사용하시길 바랍니다. 특정 검색 사이트에서 크롤링을 시작할 때는 먼저 해당 사이트에서 검색을 해보는 것이 중요합니다. 해당 사이트에서의 검색 결과가 어떻게 도출 되는지 확인하기 위해서 입니다. 2016년에 개봉한 곡성영화에 대해 검색해 봅시다. 그림과 같이 곡성군청에 관한 주소와 우리가 원하는 영화 곡성에 대한 정보가 나타납니다. 일단 단순 검색 했을 시 그림처럼 나타날 뿐 우리가 원하는 뉴스 검색 기사 수는 뉴스 탭을 클릭해보면 실제로 한국에는 곡성군 지역이 있기에 우리가 원하는 영화에 대한 기사는 한참 뒤로 가려져 있는 듯 합니다. (물론 검색한 시점이 영화가 개봉한 지 2년이나 지난…) 또한 무려 10만건의 기사수가 나타난 것을 알 수 있습니다. 그렇다면 영화명만 검색하지 말고 영화명+감독명 을 같이 검색한다면 어떻게 나타나는지 봅시다. 다행이 우리가 원했던 검색 기사 수를 받을 수 있게 되었습니다. 또한 9105건의 기사 수로 확연히 달라짐을 알 수 있게 되었습니다. 뉴스 기사 건 수는 검색결과에서 중간 즈음에 빨간색으로 표시된 부분 부분에서 알 수 있습니다. 자, 이제 크롬 개발자 도구를 이용하여 이 웹페이지를 분석해볼 것입니다. 크롬을 연상태에서 F12 키를 입력하면 개발자 도구가 나타납니다. 음.. 뭔가 복잡해 보이지만 우리가 사용할 도구는 왼쪽 상단에 마우스 모양의 도구를 이용할 것입니다. 마우스 도구를 이용하여 마우스를 페이지상에 옮겨 다니면 마우스가 올려진 부분에 대한 html 소스코드가 개발자 도구에서 하이라이트 쳐진 상태로 나타납니다. 여기서 우리는 뉴스 기사 수에 관한 부분으로 마우스를 올려 보면 기사 수에 대한 부분의 html 코드를 자동으로 찾아 줍니다. (갓 크롬…) 보시면 &lt;span&gt;태그 사이에 존재하고 상위에는 &lt;div class=&quot;title_desc all_my&quot;&gt;태그가 감싸고 있습니다. (메모…) 이제 네이버에서 검색한 웹 주소를 복사해 둡시다. 1https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=news&amp;query=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;oquery=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;tqi=TFtVlspySDwsstafcKhssssst4d-093118 여기까지 진행한 것은 우리가 파이썬 코드로 웹 페이지의 html 태그를 읽고 우리가 원하는 데이터만을 뽑기위한 길 찾기?작업을 한것이라고 할 수 있습니다. 크롤링할 데이터의 위치를 알아 냈으니 이제 코드를 작성해봅시다. 우선 파이썬 파일을 생성하고 12import requestsfrom bs4 import BeautifulSoup 를 입력합니다. requests 패키지를 이용하여 http 요청을 실행하고 (웹 요청) BeautifulSoup 패키지를 이용하여 html 소스를 읽을 것입니다. 이제 주소를 담은 url변수를 만들고 1234import requestsfrom bs4 import BeautifulSoupurl = \"https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=news&amp;query=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;oquery=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;tqi=TFt3wdpySD0ssvkgAFCssssstbh-278429\" requests.get을 이용하여 http get요청을 실행 할 수 있습니다. 123456import requestsfrom bs4 import BeautifulSoupurl = \"https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=news&amp;query=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;oquery=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;tqi=TFt3wdpySD0ssvkgAFCssssstbh-278429\"req = requests.get(url) print(req)를 해보면 응답코드를 확인할 수 있습니다. 정상 응답은 코드가 200이면 요청이 성공한 것 입니다. 1234567import requestsfrom bs4 import BeautifulSoupurl = \"https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=news&amp;query=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;oquery=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;tqi=TFt3wdpySD0ssvkgAFCssssstbh-278429\"req = requests.get(url)print(req) 실행결과: 오류해결403응답으로 즉,서버가 요청을 거부한 상태입니다. HTTP 403 응답이란 HTTP요청을 보낼때는 Header라는 부가적인 정보를 담아 보냅니다. 서버는 이 헤더 정보를 통해 정상적인 유저의 접근인가를 확인하는데 이 헤더정보를 담아서 요청을 보내봅시다. requests.get 에 headers라는 옵션을 추가하고 변수를 추가시킵시다. 1234567891011import requestsfrom bs4 import BeautifulSoupurl = \"https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=news&amp;query=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;oquery=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;tqi=TFt3wdpySD0ssvkgAFCssssstbh-278429\"header = { 'User-Agent': 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; ko-KR))',}req = requests.get(url,headers=header)print(req) 실행결과: 드디어 정상적인 접근이 성공 했습니다. print(req) 부분을 print(req.content)로 수정시키면 페이지 코드를 찍을 수 있습니다. 이제 BeautifulSoup로 html코드를 읽읍시다. 1234567891011import requestsfrom bs4 import BeautifulSoupurl = \"https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=news&amp;query=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;oquery=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;tqi=TFt3wdpySD0ssvkgAFCssssstbh-278429\"header = { 'User-Agent': 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; ko-KR))',}req = requests.get(url, headers=header)soup = BeautifulSoup(req.content, 'html.parser') req.content는 requests를 통해 받은 페이지 정보를 가지고 있고 html.parser로 html 인코딩 시킵니다. BeautifulSoup는 여러가지 기능을 제공하는데 우리는 find옵션을 사용합니다. soup.find는 html소스로 부터 입력한 파라미터의 정보를 찾아줍니다. 위에서 우리는 &lt;div class=&quot;title_desc all_my&quot;&gt;태그 사이에 데이터를 가져올 수 있으므로 아래와 같이 파라미터를 설정합니다. 1234567891011121314import requestsfrom bs4 import BeautifulSoupurl = \"https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=news&amp;query=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;oquery=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;tqi=TFt3wdpySD0ssvkgAFCssssstbh-278429\"header = { 'User-Agent': 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; ko-KR))',}req = requests.get(url, headers=header)soup = BeautifulSoup(req.content, 'html.parser')data = soup.find('div', class_='title_desc all_my')print(data) 실행결과: 여전히 &lt;span&gt;태그로 감싸고 있습니다. 우리는 .text 를 붙여서 태그들 사이의 텍스트 정보만 가져올 수 있습니다. 1234567891011121314import requestsfrom bs4 import BeautifulSoupurl = \"https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=news&amp;query=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;oquery=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;tqi=TFt3wdpySD0ssvkgAFCssssstbh-278429\"header = { 'User-Agent': 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; ko-KR))',}req = requests.get(url, headers=header)soup = BeautifulSoup(req.content, 'html.parser')data = soup.find('div', class_='title_desc all_my').text #.text추가print(data) 실행결과: 음… 여전히 수정이 필요합니다. 거슬리는 부분을 제거하기 위해 아래 방식으로 수정합시다. 또한 에러등을 잡기 위해 try excep 문을 추가하였습니다. 123456789101112131415161718import requestsfrom bs4 import BeautifulSoupurl = \"https://search.naver.com/search.naver?sm=tab_hty.top&amp;where=news&amp;query=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;oquery=%EA%B3%A1%EC%84%B1+%EB%82%98%ED%99%8D%EC%A7%84&amp;tqi=TFt3wdpySD0ssvkgAFCssssstbh-278429\"header = { 'User-Agent': 'Mozilla/5.0 (Windows; U; MSIE 9.0; WIndows NT 9.0; ko-KR))',}try: req = requests.get(url, headers=header) soup = BeautifulSoup(req.content, 'html.parser') data = soup.find('div', class_='title_desc all_my').text data = data[7:-1] data = data.replace(',', '') print(data)except Exception as e: print(e) 실행결과: 파이썬의 data[:] 는 문자열 포맷하는 방법으로 data[시작부분:끝부분]으로 문자열을 포맷합니다. 즉 1-10 / 9,105건이라는 문자열의 앞부분 1-10 /을 건너띄고 건부분을 지우는 것입니다. 문자열을 포맷하고 나면 9,105가 남게 되는데 replace로 ,콤마를 빈값으로 바꿉니다. 따라서 최종적으로 9105 문자열이 완성되는 것입니다. unsplash-logoKristijan Arsov","link":"/2018/01/30/NaverCrawler/"},{"title":"ORA-27102 out of memory","text":"서버를 리부팅하면 가끔씩 오라클 메모리 사이즈에 오류가 생겨서 ORA-27102 와 같은 에러가 생길 수 있다. 에러 발생시 root 권한으로 접속하여 아래 명령어로 memory 크기를 확인한다. (Centos) 1$ getconf PAGE_SIZE shmall 란 값을 계산한다. $$\\frac{(Memory(GB)\\times(1024\\times1024\\times1024)}{PAGE SIZE}$$ 위에 식으로 계산한후 vi 로 /etc/sysctl.conf 를 수정한다. 123$ vi /etc/sysctl.confkernel.shmall = * 리부팅을 하거나 아래 명령어로 커널에서 읽어 오게 한다. 1$ sysctl -p PC의 메모리 크기가 1GB 거나 보다 작은 경우 서버의 특성상 램을 추가하는 것을 추천한다.","link":"/2016/05/10/ORA27102/"},{"title":"Oracle 12c release2 설치(1&#x2F;2) Centos","text":"시스템 버전: CentOS (7.1) 64 비트 운영체제 오라클 버전: Oracle 12c (12.1.0) 1. 준비 현재 리눅스 시스템에 적용된 호스트 네임을 확인합니다. 설치과정중에 tnsnames.ora 파일 설정시에 필요하므로 꼭 확인바랍니다. 1$ cat /etc/hostname SELlinux 가 활성화 되어 있는지 확인하고 활성된 상태면 설치를 진행 합니다. 12345678910$sestatusSELinux status: enabled ◀ enable 이면 활성화된 상태.SELinuxfs mount: /sys/fs/selinuxSELinux root directory: /etc/selinuxLoaded policy name: targetedCurrent mode: enforcingMode from config file: enforcingPolicy MLS status: enabledPolicy deny_unknown status: allowedMax kernel policy version: 28 만약 “disable” 로 설정되어있으면 비활성된 상태이므로 설정을 바꿔줍시다. 방화벽이 활성화 되어 있는지 확인합니다. 12$ firewall-cmd --staterunning 위와 같이 “running” 으로 나타나면 활성화 된 상태이고 “not running” 으로 나타날 경우엔 1$ systemctl enable firewalld 과 같이 입력하여 활성화 한 후 재확인을 해보시기 바랍니다. 다음으로 OS 패키지들을 최신 버전으로 업데이트 시킵니다. 1$ yum update -y 아래 링크로 접속하여 통해 Oracle 설치 파일을 다운로드 받습니다. http://www.oracle.com/technetwork/database/enterprise-edition/downloads/index.html Accept License Agreement 에 체크를 하고 File 1, File 2 를 다운로드 합니다. 그 후 설치하는데 필요한 패키지들을 아래 명령어로 설치합니다. 1234$ yum install -y binutils.x86_64 compat-libcap1.x86_64 gcc.x86_64 gcc-c++.x86_64 glibc.i686 glibc.x86_64 \\glibc-devel.i686 glibc-devel.x86_64 ksh compat-libstdc++-33 libaio.i686 libaio.x86_64 libaio-devel.i686 libaio-devel.x86_64 \\libgcc.i686 libgcc.x86_64 libstdc++.i686 libstdc++.x86_64 libstdc++-devel.i686 libstdc++-devel.x86_64 libXi.i686 libXi.x86_64 \\libXtst.i686 libXtst.x86_64 make.x86_64 sysstat.x86_64 2. 설치과정 리눅스 유저와 Oracle Database에서 사용할 그룹을 아래와 같이 생성합니다. 1234$ groupadd oinstall$ groupadd dba$ useradd -g oinstall -G dba oracle$ passwd oracle vi 에디터로 /etc/sysctl.conf 파일을 열어 아래와 같이 수정한 후 저장합니다. 1234567891011fs.aio-max-nr = 1048576fs.file-max = 6815744kernel.shmall = 2097152kernel.shmmax = 1987162112kernel.shmmni = 4096kernel.sem = 250 32000 100 128net.ipv4.ip_local_port_range = 9000 65500net.core.rmem_default = 262144net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048586 수정된 내용을 아래 명령어를 입력하여 적용 시킵니다. 12$ sysctl -p$ sysctl -a 준비 과정에서 다운로드 받은 오라클 설치 패키지를 아래 명령어로 압축 해제 시킵니다. 12$ unzip linuxamd64_12102_database_1of2.zip -d /stage/$ unzip linuxamd64_12102_database_2of2.zip -d /stage/ 압축과 동시에 /stage 라는 디렉토리가 생성되고 그안에 압축 해제된 파일들이 저장됩니다. /stage 디렉토리에 위에서 생성한 oracle 유저의 접근 권한을 아래 명령어로 설정합니다. 1$ chown -R oracle:oinstall /stage/ 두 개의 디렉토리를 생성합니다. 하나는 /u01 다른 하나는 /u02로 설정합니다. 두 디렉토리에 저장될 파일들은 아래와 같으므로 여러분이 임의의 경로에 다른 이름으로 저장하여도 무관하나 앞으로 제가 소개할 설치 과정에선 아래 두 경로를 사용할 것입니다. 12/u01 오라클 프로그램 파일들/u02 데이터베이스 테이블 파일들 두 디렉토리를 생성함과 동시에 oracle 유저에게 접근 권한을 부여합니다. 12345678$ mkdir /u01$ mkdir /u02$ chown -R oracle:oinstall /u01$ chown -R oracle:oinstall /u02$ chmod -R 775 /u01$ chmod -R 775 /u02$ chmod g+s /u01$ chmod g+s /u02 아래와 같이 oracle 계정으로 계정을 변경하여 오라클 설치파일을 실행 시킵니다. (JRE가 설치 되어 있지 않다면 오류가 발생 할 수 있습니다.) 12$ su oracle$ /stage/database/runInstaller","link":"/2016/04/12/OracleInstall_1/"},{"title":"Oracle 12c release2 설치(2&#x2F;2) Centos","text":"3. 설치 화면 Configure Security Updates 첫번째 화면에서 I wish to receive security update…. 을 선택 해제 하고 Next 를 클릭합니다. 아래 팝업이 뜨면 Yes를 선택하고 설치를 진행합니다. Installation Option Create and configure a database 를 선택하고 Next를 클릭합니다. System class Desktop Class 를 선택하고 Next를 클릭합니다. Typical Installation 다음 화면에서 여러 입력란이 있는데 각 란에 아래와 같이 입력합니다. Oracle base /u01/app/oracle Software location /u01/app/oracle/product/12.1.0/dbhome_1 Database file location /u02 Global database name orcl.korea.com Oracle base = 오라클 최상위 디렉토리 Software location = 오라클 프로그램 파일 저장 위치 Database file location = DB과련 파일 저장 위치 Global database name = 디비 서버 이름 Administrative password 에 현재 설치하는 오라클의 관리자 비밀번호를 설정합니다. 가능하면 길게, 영문, 숫자를 섞어서 입력하시길 권합니다. 중요!!!!Character Set 를 Unicode (AL32UTF8) 로 선택합니다. (Default 로하면 한글이 깨져요..) 입력을 마치고 Next를 선택합니다. Create Inventory Inventory Directory 가 /u01/app/oraInventory 임을 확인하고 Next를 클릭합니다. Prerequisite Checks 이제 설치 프로그램이 자동으로 필요한 OS 패키지 및 OS 커널 설정을 확인합니다. 이미 설치가 되었다면 오른쪽에 있는 Ignore all 을 선택하고 Next 를 클릭합니다. Summary 마지막으로 설치 설정을 확인하고 Install을 선택합니다. Execute Configuration Script 만약 request window 가 뜬다면 root 로 로그인 한 후 아래 두 명령어를 실행합니다. 1234567$ /u01/app/oraInventory/orainstRoot.shChanging permissions of /u01/app/oraInventory.Adding read,write permissions for group.Removing read,write,execute permissions for world.Changing groupname of /u01/app/oraInventory to oinstall.The execution of the script is complete. 12345678910111213141516$ /u01/app/oracle/product/12.1.0/dbhome_1/root.shPerforming root user operation.The following environment variables are set as: ORACLE_OWNER= oracle ORACLE_HOME= /u01/app/oracle/product/12.1.0/dbhome_1Enter the full pathname of the local bin directory: [/usr/local/bin]: &lt;PRESS ENTER&gt; Copying dbhome to /usr/local/bin ... Copying oraenv to /usr/local/bin ... Copying coraenv to /usr/local/bin ...Creating /etc/oratab file...Entries will be added to the /etc/oratab file as needed byDatabase Configuration Assistant when a database is createdFinished running generic part of root script.Now product-specific root actions will be performed.You can follow the installation in a separated window. 모든 설치 구성사항을 설정했습니다.Installation progress가 뜨면 설치 설정이 완료 된것이고 설치 진행이 됩니다. Installation progress가 끝나면 Oracle 12c r2 설치가 완료 된겁니다. 수고하셨습니다.","link":"/2016/04/12/OracleInstall_2/"},{"title":"Hadoop Clustering (2)","text":"오픈소스 Hadoop 배포판을 다운로드하여 설치 및 설정에 대해서 알아보겠습니다. 아래는 Apache Hadoop 배포판 한국 미러링 사이트입니다. Hadoop Distribution Mirror KR wget 명령을 설치하여 다운로드 링크 주소를 입력하여 리눅스에 바로 다운받았다 파일명을 읽어보면 src가 붙은게 있고 없는게 있는데 소스파일이란 뜻이다 src 로 받을 시 make 로 빌드까지 해야 하는 번거로움이 있으므로 바이너리 파일로 받는다 123yum install wget -ywget http://mirror.apache-kr.org/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gz Hadoop ConfigurationEnvironment Configuration File아래 site-specific configuration files 를 수정하여 Hadoop 클러스터의 구성을 설정할 수 있다 Site-Specific Configuration Files core-site.xml Core Hadoop configuration mapred-site.xml Configuration for Mapreduce hdfs-site.xml Configuration for HDFS yarn-site.xml Configuration for YARN 추가적으로 설정할 수 있는 파일들은 다음과 같다 Other Hadoop-Related Configuration Files log4j.properties for configuring logging hadoop-metric.properties Configuration for hadoop metrics allocations.xml for configuring Fair Scheduler capacity-scheduler.xml for configuring Capacity Scheduler incude and exclude files Used for specifying which hosts to includeor exclude from a hadoop cluster Hadoop의 로깅 설정은 log4j.properties를 수정하여 변경할 수 있는 등 여러가지 추가 설정파일들이 많다 자세한건 다음에 다시 다루도록 하자 unsplash-logoTobias Adam","link":"/2018/08/12/Proxmox_Hadoop_2/"},{"title":"Hadoop Clustering (1)","text":"앞으로 진행할 내용은 6개의 node로 구성된 multi-node distributed Hadoop cluster 를 구성하는 것입니다. HDP, CDH와 같은 Hadoop 배포판을 사용하지 않고 aphache.org의 Native Hadoop 배포판으로 설치를 진행합니다. Hadoop 클러스터링 환경을 구성하기 전에 아래 처럼 노드들을 구상하였습니다. node host address 1 lxc-centos-1 10.10.30.101 2 lxc-centos-2 10.10.30.102 3 lxc-centos-3 10.10.30.103 4 lxc-centos-4 10.10.30.104 5 lxc-centos-5 10.10.30.105 6 lxc-centos-6 10.10.30.106 각 호스트별로 설정하기 번거로우니 node 1번에 기본설정을 한후 나머지 노드들을 clone하기로 한다 그리고 본인이 다른 용도로 운영중인 Ubuntu 18.04 데스크탑도 차후에 worker node로 추가하기로 했다 Preparationyum update, SSH 설정1234567891011121314yum update -yyum install epel-release net-tools vim openssh-server ntp pssh # 외부 SSH접속 허용echo PermitRootLogin yes &gt;&gt; /etc/ssh/sshd_configsystemctl start sshdsystemctl enable sshdssh-keygen -t rsa# Enter 두번# ssh localhostssh-keygen -t rsa#엔터 연타 Java123456yum install java-1.8.0-openjdk-develjava -version# JAVA_HOME 환경변수 등록 스크립트echo \"export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.el7_7.x86_64\" &gt; /etc/profile.d/java.sh File open limits 변경리눅스에는 동시에 파일을 open 하거나 실행 할 수 있는 갯수에 대한 제한, open file limit이 걸려있다 아래와 같이 리소스 제한을 풀어준다 123456789101112131415161718# 최소ulimit -Sn# 1024# 최대ulimit -Hn# 524288# 아래 파일을 수정vim /etc/security/limits.confsoft nofile 32768hard nofile 32768# 커널 적용sysctl -p# 확인sysctl -a NTP 설정123456789101112131415# 현재 서버 시간 및 time zone 확인timedelta# UST로 되어 있을 시 아래 실행해서 KST로 timezone 설정ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtimevim /etc/ntp.conf# 기존의 server들을 주석처리# server 0.centos.pool.ntp.org.iburst# server 0.centos.pool.ntp.org.iburst# server 0.centos.pool.ntp.org.iburst# server 0.centos.pool.ntp.org.iburstserver 127.127.1.0# 저장 후 ntpd 데몬 시작systemctl start ntpd 우선 lxc-centos-1 을 ntp 서버로 두고 차후 클러스터들로 하여금 시간 동기화를 진행하도록 한다 호스트 파일 설정123456789vim /etc/hosts10.10.30.101 lxc-centos-110.10.30.102 lxc-centos-210.10.30.103 lxc-centos-310.10.30.104 lxc-centos-410.10.30.105 lxc-centos-510.10.30.106 lxc-centos-6#저장 Disable THPCDH나 HDP에서 언급하길 Transparent Huge Pages(THP)의 THP compaction로 인해 Hadoop의 성능이 저하될 수 있다고 한다. pve 서버에 접속하여 아래 명령을 입력하자, 컨테이너는 커널을 공유하기 때문에 컨테이너상에서 커널 설정을 넣어도 적용되지 않기 때문에 메인 커널 서버에 접속해주고 설정을 잡아줘야 한다. THP 옵션 활성화 상태 확인 1cat /sys/kernel/mm/transparent_hugepage/enabled always가 []로 싸여서 [always] 로 되어있으면 활성된 상태이다 다음 2가지 절차로 THP를 끌 수 있다 아래 echo 명령어를 /etc/rc.d/rc.local 에 덮어 씌우고 reboot 1234echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defragchmod +x /etc/rc.d/rc.local /etc/default/grub 파일의 GRUB_CMDLINE_LINUX 옵션에 다음 내용을 추가한다 1234GRUB_CMDLINE_LINUX = \"transparent_hugepage=never\"# 내용 추가 후 아래 실행grub2-mkconfig -o /boot/grub2/grub.conf 그 후 cat /proc/meminfo 를 열어서 아래 처럼 모든 값이 0이 되었는지 확인해준다 12345AnonHugePages: 0 kBHugePages_Total: 0HugePages_Free: 0HugePages_Rsvd: 0HugePages_Surp: 0 더 자세한 내용은 아래 링크에서 확인하자 What is huge page in linux? vm.swappiness 설정CDH, HDP에 따르면 swap 이 설정되어 있다면 메모리를 최대한 활용하지 못하는 경우가 생길 수 있기 때문에 되도록이면 끄거나 최소크기로 지정하는 것으로 권장 되어있다 pve 서버에 접속해서 아래 내용들을 수행한다 12345# swap 크기 확인cat /proc/sys/vm/swappinesstouch /etc/sysctl.d/set-swap-1.confecho \"vm.swappiness=1\" &gt; /etc/sysctl.d/set-swap-1.conf 얼추 필요한 프로그램과 설정이 끝났으니 이제 clone을 시켜준다 Proxmox web-ui에 접속해서 lxc-centos-1 컨테이너에 오른쪽 버튼을 클릭하고 (꺼진 상태여야함) clone을 눌러서 간단히 hostname만 입력해주면 복제된 컨테이너를 생성할 수 있다 이제 각 노드에 들어가서 ssh-copy-id, IP 변경, ntp 동기화를 하도록 하자 pssh를 설치 하였기 때문에 모든 노드들에게 병렬로 동일한 명령을 내릴 수 있다 pssh -h [호스트파일] [명령어] 1234567891011vim hostslxc-centos-1lxc-centos-2lxc-centos-3lxc-centos-4lxc-centos-5lxc-centos-6#저장pssh -h hosts yum update -y 추가 proxmox-6.1에서 LXC가 stop이나 reboot, clone 시 /etc/hosts가 초기설정으로 override 되는 문제가 있어서 아래를 실행해준다 1touch /etc/.pve-ignore 이러면 /etc/hosts가 초기 설정으로 바뀌는걸 막을 수 있다 unsplash-logoMylon Ollila","link":"/2018/08/11/Proxmox_Hadoop_1/"},{"title":"Proxmox 설치","text":"Citrix사의 Xenserver를 사용하면서 느낀것은 VM을 구동과 운영에 간편한 API와 서비스를 제공하지만 클라이언트 프로그램(Xen Center)이 Windows 운영체제 버전만 제공하기에 불편한점이 이만전만 아니었다.(필자는 Mac유저…) 또한 GPU passthrough를 지원하기 위해선 추가적으로 subscription이 필요하다는 것을 알았다 (가격도 만만치 않다) 그래서 KVM 기반의 하이퍼바이저 제품을 조사하던중 Proxmox라는 제품이 눈에 들어왔다. Proxmox는 Web-UI를 제공하기에 언제 어디서든 브라우저만 있다면 관리 화면을 접속할 수 있고 GPU-Passthrough를 무료로 지원한다. 해외에서는 homelab으로 가상화 시스템을 구축하는데 많이 사용하는 것으로 나타난다. 구글 트랜드에서 Proxmox, Xenserver, Vmware vsphere로 검색을 해보면 Proxmox에 대한 관심도는 꾸준히 상승하는 것으로 보인다. Proxmox 홈페이지 오픈소스 KVM 하이퍼바이저로는 많이 각광받는편이다. ISO이미지를 rufus로 USB에 담기하이퍼바이저 설치는 USB에 ISO를 설치하면 진행한다. USB로 설치홈페이지에 Downloads 메뉴를 통해 설치 ISO이미지를 다운받을 수 있었다 최신버전인 6.1버전을 다운받기로 하였다 다음엔 ISO이미지를 USB에 담기 위해 rufus를 사용하기로 하였다 rufus 홈페이지 무료로 다운받을 수 있다. Windows에서만 구동되므로 참고하자 부팅에 성공한 화면이다 첫번째 install Proxmox VE를 두고 Enter 라이센스화면은 I agree로 넘어가주고 설치할 디스크를 선택해주는 화면이다 /dev/sda로 마운트된 SSD에 설처하기로 한다 Timezone을 설정해준다 랜선이 꽂혀있는 상태여서인지 자동으로 한국시간대로 잡혔다 root계정 비밀번호와 이메일을 설정해준다 네트워크 설정 화면이다 FQDN은 아무이름으로 설정하고 나머지는 적절한 IP를 할당해주기로한다 설정한 정보들을 summary로 보여준다 install를 눌러서 설치를 진행한다 설치가 진행되는 화면이 나오고 잠시 기다리면 설치가 완료되었음을 알린다 Reboot를 눌러서 재부팅해주자 재부팅이 완료되면 그림과 같이 Web-UI로 접속할 수 있는 주소가 나타나게 된다 네트워크 설정해서 넣은 서버 IP 주소에 port번호는 4000으로 잡혔다 https로 시작하는 것에 유의해야 한다 Web-UI에서 접속이제 브라우저로 넘어와서 접속을 해보자 유효하지 않은 인증서가 뜨게 되면 무시하고 자세히 보기에 들어가서 해당 링크로 강제 접속을 하자 root 계정으로 접속해주자 구독권이 없다고 나오는데 이것은 proxmox 유료구독권을 구매해야하는 것 같다 하지만 서비스 이용엔 문제가 없으니 pass 메인 화면이 나왔다 나름 깔끔한 UI를 제공하는 것 같다 unsplash-logoKvistholt Photography","link":"/2019/12/28/Proxmox_install/"},{"title":"Proxmox VM에 Hadoop 설치하기","text":"HDD Mount현재 테스트중인 서버엔 128GB SSD하나, 2TB HDD가 달려있고 proxmox는 SSD에 설치되었다 Proxmox 설치 당시 /dev/sda에 마운트된 SSD에 설치하여 Web-UI의 Disk화면에서 조회가 가능하나 2TB의 HDD가 마운트된 상태가 아니었다 HDD를 마운트시키고 앞으로 올릴 컨테이너의 데이터를 저장하도록 하자 123456789101112fdisk -l ## 마운트 위치, 파티션 확인mkfs.ext4 /dev/sdb1# /dev/sdb1 is apparently in use by the system; will not make a filesystem here! 에러시# dmsetup statusdmsetup remove_allmkfs.ext4 /dev/sdb1 # proceed anyway? yfdisk -l 포맷 과정에서 fdisk -l에 기존 Xenserver의 LVM이 매핑되어 있어서 관련정보를 dmsetup 명령어로 정리해주니 깔끔히 포맷이 되었다 지금까지는 Linux상에서 디스크 포맷, 마운트를 진행하였고 Proxmox에서 인식하기 위해서는 아래 명령어를 입력해준다 vgcreate 명령입력시 Proxmox에서 인식할 이름 태그을 붙여준다. 일단 newdrive라고 붙여줬다 12pvcreate /dev/sdb1vgcreate newdrive /dev/sdb1 다시 Proxmox의 Web-UI로 돌아와서 아래와 같이 들어가준다 아래와 같이 창이뜨면 ID를 식별하기 쉬운 이름으로 하고 Volume Group으로 설정하면 끝난다 LXC 생성Template Download생성할 LXC의 ISO를 Proxmox에서 제공하는 Template 을 받으면 실행할 수 있다 웹브라우저로 Proxmox 관리화면을 들어가보면 위 와 같이 local - Content 에 보면 Template 항목을 눌러보면 다운받을 수 있는 다양한 Template을 제공한다 일단 Centos-7 버전을 다운받기로 했다 다운로드가 시작되었다 한 네트워크환경에 따라 속도가 다르지만 10분정도가 소요되었다 Starting LXC 메인화면으로 돌아와서 오른쪽 상단에 LXC생성 버튼을 눌러준다 ID는 100으로 시작하지만 101로 주고 설정하기로 한다 Hostname 과 root password 를 입력해주고 Next 조금전에 다운받은 Template 으로 설정해줬다 HDD 설정이다 우선 disk 사이즈를 300으로 잡아줬고 아래에 Adcanced 옵션을 눌러보니 세부 옵션을 추가할 수 있는데 I/O성능을 올릴 수 있는 noatime 을 설정해줬다 CPU는 넉넉히 4코어로 잡아주었다 메모리는 8GB 로 잡아줬다 네트워크 화면이다, IPv4에 고정 IP를 잡아줬고 firewall 를 체크해제 해줬다 DNS설정이다 가장 빠른 DNS서버 주소를 잡아줬다 현재 까지 설정한 내역이다 별 어려움없이 생성이 끝났다 앞으로 해당 컨테이너에 필요한 설정들을 집어넣고 Clone 시켜서 Hadoop cluster를 구성할 계획이다 unsplash-logoTaylor Vick","link":"/2019/12/29/Proxmox_LXC/"},{"title":"SPARK 클러스터 환경설정","text":"Prerequisits : Linux (Ubuntu, Centos) JAVA 설치된 상태 SSH 설치 SPARK 다운로드 Master PC, Slave PC 설정 /usr/local에 다운받은 SPARK를 압축해제 시킨다. (Slave 도) 1$ tar xvf spark-* 폴더명을 spark 로 바꾼다. 일반 유저에게 권한을 부여한다.(전체 노드에 설정) 1$ mv SPARK-* spark 1$ chown -R (유저명):(그룹명) spark 환경변수에 자바와,스파크 디렉토리를 등록한다. (전체 노드에 설정) 123456$ vi .bashrcexport $JAVA_HOME=/usr/loca/java (알아서 찾길)export $SPARK_HOME=/usr/local/sparkexport PATH=$PATH:$SPARK_HOME/bin$ source .bashrc /etc/hosts 에 마스터 ip 와 슬레이브 ip 를 입력한다. 123$ vi /etc/hosts192.xxx.xxx.xx master192.xxx.xxx.xx slave01 서로간에 ssh 가 통하는지 확인한다. 안돼면 포트 문제거나 설치가 안된거 12$ ssh slave01$ ssh slave02 마스터 PC의 SPARK 디렉토리의 conf/slaves.template 수정, 슬레이브 호스트명 입력 1234$ vi slave.templateslave01slave02 마스터 PC의 SPARK 디렉토리에 sbin/start-all.sh 실행. 브라우저 localhost:8080 로 들어가서 연결확인* 출처 unsplash-logoJake Blucker","link":"/2017/10/18/SparkCluster/"},{"title":"Use integrated graphics for display - Ubuntu","text":"NVIDIA 드라이버 설치 및 설정을 마치면 정상적으로 X Windows 환경을 사용할 수 있게 되는데, X Windows가 그래픽 카드를 사용하면서 메모리를 조금 잡아먹게 된다. X Windows 처리를 CPU(intel) 내장그래픽으로 처리하는 방법에 대해 알아보았다. xorg.conf 수정X Windows가 CPU(Intel) 내장 그래픽을 사용하게 하기 위한 방법은 다음과 같다 . CPU가 내장그래픽을 지원하는지 확인!!! 터미널로 접속하고 /etc/x11/xorg.conf 를 수정해준다. 만약 없으면 xorg.conf.xxx 파일이 같은 경로에 있을 텐데 cp 로 복사본을 만들자 1sudo vi /etc/x11/xorg.conf 12345678910111213141516171819202122232425262728293031Section \"ServerLayout\" Identifier \"layout\" Screen 0 \"nvidia\" ## \"intel\" 로 수정 inactive \"intel\" ## 지워준다EndSectionSection \"Device\" Identifier \"intel\" Driver \"modesetting\" BusID \"PCI:0@0:2:0\" Option \"AccelMethod\" \"None\"EndSectionSection \"Screen\" Identifier \"intel\" Device \"intel\"EndSectionSection \"Device\" Identifier \"nvidia\" Driver \"nvidia\" BusID \"PCI:1@0:0:0\" Option \"ConstrainCursor\" \"off\"EndSectionSection \"Screen\" Identifier \"nvidia\" Device \"nvidia\" Option \"AllowEmptyInitialConfiguration\" \"on\" Option \"IgnoreDisplayDevices\" \"CRT\"EndSection ServerLayout 부분을 자세히 보면 Screen 0 &quot;nvidia&quot; inactive &quot;intel&quot; 이 있는데 Screen 0 를 &quot;intel&quot; 로 inactive 부분은 지워주도록 한다. 1234Section \"ServerLayout\" Identifier \"layout\" Screen 0 \"intel\"EndSection wq! 로 저장하고 Reboot 해줍시다 위 예시는 싱글 모니터를 사용할때의 경우이고 만약 듀얼 스크린을 쓴다면 Screen 1 도 있을텐데 &quot;intel&quot; 로 수정해주자 재부팅하고 nvidia-smi 를 확인해보면 그래픽 카드의 메모리를 사용하지 않는것을 확인할 수 있다. unsplash-logoDaniel Leone","link":"/2019/12/27/Use_integrated_graphics_for_display%20-%20Ubuntu/"},{"title":"Object Detection with YOLOv3","text":"SW dependencies NVIDIA Graphic Driver CUDA-Toolkit &gt;= 7.5 (GPGPU 연산) 5.x &lt;= cuDNN &lt;= 7.x (DL 고성능 연산용) 3.x &lt;= OpenCV &lt;= 3.4.0 (미디어 포맷 인식 및 결과 Grid 그리기) CMake &gt;= 3.8 (C++ 소스코드 빌드) Test Environment CPU Intel Xeon(R) CPU E5-2620 16core(8x2) 2.1GHz RAM 128G GPU Titan X (Pascal) (*1ea), computer capability : 6.1 OS Ubuntu 16.04 (Xenial Xerus) 1. Container화테스트 실행에 필요한 환경을 컨테이너로 압축하여 구성한다. 인계받은 자료에 의하면 소스 코드 빌드를 마치고 1./DeepVisionXPlayer [cfg file] [Data cfg file] [weight file] [테스트 미디어 파일] 으로 파일을 실행한다. 우선 요구환경을 아래와 같이 순서대로 컨테이너화 하여 구축한후 아래처럼 [테스트 미디어 파일]을 argument로 설정하여 실행하도록 한다. 1docker run yolo:v1 [테스트 미디어 파일] 컨테이너화 단계 중 OpenCV3.4 를 활용하면 Faster RCNN이나 SSD등의 객체 인식 프레임워크 테스트 환경으로도 활용할 수 있을 것으로 예상된다. 1.1 NVIDA 드라이버 설치Nouveau 관련 설정 Linux상에서 NVIDIA 그래픽 드라이버 설치시 nouveau라는 그래픽드라이버가 충돌을 일으키므로 해제하라는 경고 메시지를 만날 수 있다. nouveau는 Linux에 기본적으로 내장된 그래픽 드라이버로 이를 커널로 부터 해제시켜야 정상적으로 설치를 진행할 수 있다. Centos7 상에서는 grub에 설정파일의 GRUB_CMDLINE_LINUX항목 끝에 아래 내용을 입력하여 해결할 수 있다. 123vi /etc/default/grub---GRUB_CMDLINE_LINUX=\".... rd.driver.blacklist=nouveau nouveau.modeset=0\" 내용을 입력하였으면 저장wq!하고 커널의 설정을 변경하였으므로 변경된 내용을 아래 명령어로 반영시킨다. 123grub2-mkconfig -o /boot/grub2/grub.cfgreboot ## 수정된 커널로 재부팅 드라이버 설치 Nvidia 드라이버 홈페이지로 부터 드라이버 설치를 진행한다. 설치할 서버의 GPU 장비에 맞는 버전을 찾아 설치한다. 1wget http://kr.download.nvidia.com/XFree86/Linux-x86_64/390.77/NVIDIA-Linux-x86_64-390.77.run 다운로드된 설치파일은 실행권한이 없으므로 아래와 같이 권한을 부여한다. 1chmod a+x NVIDIA-Linux-x86_64-390.77.run 실행권한이 부여되었으므로 설치를 진행한다. 1sh NVIDIA-Linux-x86_64-390.77.run 설치가 완료되면 아래 쉘 명령어로 GPU 장비 상태를 확인한다. 1nvidia-smi -L 1.2 Docker / Nvidia-docker 설치아래 yum 명령어로 간단하게 docker 설치가 가능하다 1yum install docker docker 설치가 완료되면 아래 명령어를 통해 docker가 설치된 정보를 확인할 수 있다. 1docker info 아래 명령어를 통해 docker로 생성되는 컨테이너들의 위치를 확인할 수 있다. 1docker info | grep 'Docker Root Dir' 기본적으로 /var/lib/docker위치에 컨테이너를 생성하는데 컨테이너가 많거나 무거워질 경우를 대비해충분한 공간이 있는 파티션의 디렉토리로 옮기기로 한다.현재 /data 디렉토리에 3TB 하드 두개를 Raid 0 포맷으로 mount 시켰기 때문에 해당 마운트지점에 컨테이너를 저장할 경로를 생성한다. 1mkdir /data/docker-img 그 후, docker의 root dir 변경을 위해 아래 처럼 /etc/sysconfig/docker 파일을 열어서 1vim /etc/sysconfig/docker OPTIONS 값 앞에 아래 처럼 --graph와 --storage-driver 옵션을 추가해준다. OPTIONS=’–selinux-enabled –log-driver=journald –signature-verification=false –graph=/data/docker-img –storage-driver=overlay‘ 설정이 변경되었으므로 docker 서비스를 재시작시켜준다. 1systemctl restart docker 컨테이너 저장경로가 변경되었는지 확인한다. 1docker info | grep 'Docker Root Dir' 테스트를 위해 간단한 컨테이너를 아래 처럼 생성해볼 수 있다. 1docker run hello-world 기본 docker 만으로는 서버에 장착된 그래픽카드를 인식할 수 없다. NVIDIA에서 개발한 NVIDIA-Docker를 이용하여 컨테이너를 생성하면 그래픽카드를 container에서 인식할 수 있다. NVIDIA-Docker설치는 홈페이지에 나오는 instruction을 따르기로 한다. 아래 쉘 명령어 조합들을 따라서 입력해준다. 123456789distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | \\sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo sudo yum install -y nvidia-container-runtime-hooksudo mkdir -p /usr/libexec/oci/hooks.decho -e '#!/bin/sh\\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" exec nvidia-container-runtime-hook \"$@\"' | \\sudo tee /usr/libexec/oci/hooks.d/nvidiasudo chmod +x /usr/libexec/oci/hooks.d/nvidia 입력이 완료되면 아래 docker 명령어로 docker상에서 nvidia-smi명령어 실행을 확인할 수 있다. 1docker run --rm nvidia/cuda nvidia-smi 1.3 CUDA (9.x) 및 cudnn7 컨테이너화Public Registry인 Docker Hub로 부터 CUDA 및 cudnn이 설치된 컨테이너를 pull할 수 있다.현재 테스트를 위해서 9.0-cudnn7-runtime 버전의 이미지를 생성하도록한다. 1docker pull nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04 생성된 이미지로 컨테이너를 생성하여 bash터미널로 접속한다. 1docker run -it nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04 /bin/bash 컨테이너의 OS버전과 그래픽드라이버를 확인한다. 12cat /os-releasenvidia-smi CUDA 설치를 위해선 아래 명령어를 입력한다. 1nvcc -V 1.4 CMake 컨테이너화컨테이너 나가기: shift+p + shift+q YOLOv3이 CMake 3.8+ 이상의 버전을 요구하므로 공식 release 버전중 하나인 3.11.4버전을 설치해본다.아래처럼 Dockerfile 파일을 생성한다. 123mkdir cmake-dockercd cmake-dockervi Dockerfile 123456789101112131415FROM nvidia/cuda:9.0-cudnn7-devel## change here for diff versionENV CMAKE_VERSION=3.11.4## CmakeRUN apt-get update -y &amp;&amp; apt-get upgrade -y \\ &amp;&amp; apt-get install curl wget unzip -y \\ &amp;&amp; curl https://cmake.org/files/v3.11/cmake-${CMAKE_VERSION}-Linux-x86_64.sh -o /tmp/curl-install.sh \\ &amp;&amp; chmod u+x /tmp/curl-install.sh \\ &amp;&amp; mkdir /usr/bin/cmake \\ &amp;&amp; /tmp/curl-install.sh --skip-license --prefix=/usr/bin/cmake \\ &amp;&amp; rm /tmp/curl-install.shENV PATH=\"/usr/bin/cmake/bin:${PATH}\" docker build 명령을 통해 작성한 Dockerfile로 이미지를 생성한다. 1docker build [Dockerfile 경로] -t [이미지명]:[태그] 1docker build cmake-docker -t cmake:3.11.4 생성된 이미지로 cmake 버전을 확인한다. 1docker run -it --rm cmake:3.11.4 cmake --version 1.4 OpenCV 컨테이너화디렉토리를 새로 생성하여 아래와 같이 Dockerfile을 만든다. 12mkdir opencv-dockercd docker &amp;&amp; vi Dockerfile OpenCV의 버전은 3.4.0을 설치한다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134FROM cmake:3.11.4## change here to diff versionENV OPENCV_VER=3.4.0RUN apt-get updateRUN apt-get install -y \\ wget \\ git \\ unzip \\ build-essential \\ curl \\ ## For OpenCV libgtk2.0-dev \\ pkg-config \\ libavcodec-dev \\ libavformat-dev \\ libswscale-devRUN apt-get clean## Change DirectoryWORKDIR /tmp## build and install OpenCVRUN wget https://github.com/opencv/opencv/archive/${OPENCV_VER}.zipRUN unzip ${OPENCV_VER}.zipRUN mkdir opencv-${OPENCV_VER}/buildWORKDIR opencv-${OPENCV_VER}/build## lets do thisRUN cmake \\-D CMAKE_BUILD_TYPE=Release \\-D CMAKE_INSTALL_PREFIX=/usr/local \\-D BUILD_CUDA_STUBS=OFF \\-D BUILD_DOCS=OFF \\-D BUILD_EXAMPLES=OFF \\-D BUILD_JASPER=OFF \\-D BUILD_JPEG=OFF \\-D BUILD_OPENEXR=OFF \\-D BUILD_PACKAGE=ON \\-D BUILD_PERF_TESTS=OFF \\-D BUILD_PNG=OFF \\-D BUILD_SHARED_LIBS=ON \\-D BUILD_TBB=OFF \\-D BUILD_TESTS=OFF \\-D BUILD_TIFF=OFF \\-D BUILD_WITH_DEBUG_INFO=ON \\-D BUILD_ZLIB=OFF \\-D BUILD_WEBP=OFF \\-D BUILD_opencv_apps=ON \\-D BUILD_opencv_calib3d=ON \\-D BUILD_opencv_core=ON \\-D BUILD_opencv_cudaarithm=OFF \\-D BUILD_opencv_cudabgsegm=OFF \\-D BUILD_opencv_cudacodec=OFF \\-D BUILD_opencv_cudafeatures2d=OFF \\-D BUILD_opencv_cudafilters=OFF \\-D BUILD_opencv_cudaimgproc=OFF \\-D BUILD_opencv_cudalegacy=OFF \\-D BUILD_opencv_cudaobjdetect=OFF \\-D BUILD_opencv_cudaoptflow=OFF \\-D BUILD_opencv_cudastereo=OFF \\-D BUILD_opencv_cudawarping=OFF \\-D BUILD_opencv_cudev=OFF \\-D BUILD_opencv_features2d=ON \\-D BUILD_opencv_flann=ON \\-D BUILD_opencv_highgui=ON \\-D BUILD_opencv_imgcodecs=ON \\-D BUILD_opencv_imgproc=ON \\-D BUILD_opencv_java=OFF \\-D BUILD_opencv_ml=ON \\-D BUILD_opencv_objdetect=ON \\-D BUILD_opencv_photo=ON \\-D BUILD_opencv_python2=OFF \\-D BUILD_opencv_python3=OFF \\-D BUILD_opencv_shape=ON \\-D BUILD_opencv_stitching=ON \\-D BUILD_opencv_superres=ON \\-D BUILD_opencv_ts=ON \\-D BUILD_opencv_video=ON \\-D BUILD_opencv_videoio=ON \\-D BUILD_opencv_videostab=ON \\-D BUILD_opencv_viz=OFF \\-D BUILD_opencv_world=OFF \\-D CMAKE_BUILD_TYPE=RELEASE \\-D WITH_1394=ON \\-D WITH_CUBLAS=OFF \\-D WITH_CUDA=OFF \\-D WITH_CUFFT=OFF \\-D WITH_EIGEN=ON \\-D WITH_FFMPEG=OFF \\-D WITH_GDAL=OFF \\-D WITH_GPHOTO2=OFF \\-D WITH_GIGEAPI=ON \\-D WITH_GSTREAMER=ON \\-D WITH_GTK=ON \\-D WITH_INTELPERC=OFF \\-D WITH_IPP=ON \\-D WITH_IPP_A=OFF \\-D WITH_JASPER=OFF \\-D WITH_JPEG=ON \\-D WITH_LIBV4L=OFF \\-D WITH_OPENCL=ON \\-D WITH_OPENCLAMDBLAS=OFF \\-D WITH_OPENCLAMDFFT=OFF \\-D WITH_OPENCL_SVM=OFF \\-D WITH_OPENEXR=OFF \\-D WITH_OPENGL=ON \\-D WITH_OPENMP=OFF \\-D WITH_OPENNI=OFF \\-D WITH_PNG=ON \\-D WITH_PTHREADS_PF=OFF \\-D WITH_PVAPI=ON \\-D WITH_QT=OFF \\-D WITH_TBB=ON \\-D WITH_TIFF=OFF \\-D WITH_UNICAP=OFF \\-D WITH_V4L=OFF \\-D WITH_VTK=OFF \\-D WITH_WEBP=OFF \\-D WITH_XIMEA=OFF \\-D WITH_XINE=OFF \\..RUN cmake --build . --config ReleaseRUN make -j${nproc} installRUN cd /RUN rm -rf /tmp/*# config pathRUN LD_LIBRARY_PATH=/usr/local/lib:/usr/lib; export LD_LIBRARY_PATHRUN ldconfig 1docker build opencv-docker -t opencv:3.4.0 여기까지가 컨테이너상에 CUDA + CUDNN + Cmake + Opencv 를 설치하는 과정이다.이제 YOLOv3의 소스를 받아 실행해본다. 1.5 YOLOv3Opencv컨테이너를 생성하여 쉘로 접속한다. 1docker run -it --rm opencv:3.4.0 /bin/bash 아래 명령어를 통해 Yolov3를 깃허브로부터 받는다. 1git clone https://github.com/AlexeyAB/darknet.git 다운로드가 끝나면 이제 빌드를 해야하는데 Makefile를 편집기로 열어서 (vi 없으면 설치..) 수정해준다.맨 상단에 GPU,CUDNN, OPENCV등의 옵션이 0으로 지정되어 있는데 이들을 1로 수정해준다. 12345678GPU=1CUDNN=1CUDNN_HALF=0OPENCV=1AVX=0OPENMP=0LIBSO=0... darknet폴더로 이동하여 빌드를 아래 명령어로 수행한다. 12cd darknetmake -j${nproc} 빌드가 끝나면 weight파일을 생성해야하는데 학습된 weight 샘플을 아래처럼 받을 수 있다. 1wget https://pjreddie.com/media/files/yolov3.weights 가중치에 대한 cfg는 소스에 포함되어 있으므로 아래처럼 data폴더 밑의 dog.jpg이미지에 대학 객체 인식을 수행한다. 1./darknet detect cfg/yolo.cfg yolov3.weights data/dog.jpg 결과를 조회해보면 JPG 파일 하나에서 객체를 추측하는데 0.02초 정도로 나타난다.또한 인식한 객체들에 대한 결과가 나타나고 Grid된 좌표에 대한 결과가 나타난다. 마지막에 GTK 경고가 나타나는데 이는 이미지를 보여줄 Window환경이나 Display가 찾을 수 없음을 의미한다. 경고 메시지가 닫히면 로컬에 predictions.png파일이 생성되고 수행결과를 확인해볼 수 있다. 1ls ./ 단일 이미지파일에 대해서는 객체인식을 하는것으로 나타나므로 Video파일에 대한 객체인식을 수행한다.테스트할 Video데이터는 carpark.mp4로 주차장 녹화 데이터이다. 1/darknet detector demo cfg/coco.data cfg/yolov3.cfg /tmp/yolov3.weights /tmp/carpark.mp4 뉴럴네트워크 구성에는 성공하였으나 Video 스트림에는 실패한것으로 보인다. 로컬에서도 파일생성에 실패한것으로 나타난다. 1.6 DeepVisionX Player 테스트DeepVisionX Player는 darknet 라이브러리를 이용하여 OpenCV 및 CUDA를 사용하여 YOLOv3으로 동영상, /assets\\yolov3으로부터 실시간 객체를 탐지하는 프로그램이다. 인계받은 DeepVisionX Player의 소스코드를 container로 옮긴다. 1cp -r /tmp/deep-vision-x-player/ / 빌드를 위해 build디렉토리를 생성하고 컴파일을 진행한다. 1234mkdir buildcd build cmake ..make -j${nproc} 빌드에 성공하였다면 video 파일에 대한 객체인식을 진행하고 명령어 입력 방식은 아래와 동일하다 1build/bin/DeepVisionXPlayer [yolo cfg 파일] [train cfg 파일] [수행할 video 파일] 테스트를 위해 실행한 명령어는 아래와 같다 1234## 소스파일 디렉토리로 이동한다.cd /deep-vision-x-playerbuild/bin/DeepVisionXPlayer cfg/yolov3.cfg cfg/coco.data /tmp/yolov3.weights carpark.mp4 수행결과: 마찬가지로 뉴럴 네트워크 구성에는 성공하였으나 video 파일 인식에는 실패하였다. 2. Ubuntu 16.04 DesktopGPU서버의 Host OS를 Ubuntu 16.04로 재설치하여 진행한다. OpenCV, Nvidia 그래픽 드라이버를 잡는 방법은 컨테이너화 작업때의 스크립트를 재사용하고CUDA-Toolkit, cuDNN 설치는 추가적인 작업을 진행하도록 한다. 2.1 CUDA-ToolKit 9.0, cuDNN 7 설치우선 CUDA-Toolkit을 설치하는데 최신 버전인 9.2버전을 설치하지 않고 9.0 버전을 설치한다. 아래 그림과 같이 Ubuntu 16.04 버전에 해당하는 설치파일을 다운로드 받는다. 다운로드한 CUDA 설치 파일 경로로 이동하여 실행시킵니다. 1234cd ~/Downloadschmod a+x cuda_9.0~.runsudo sh cuda_9.0~.run 설명문이 나오면 Enter 혹은 space 를 입력하여 끝까지 읽고 (Ctrl + c 로 스킵 가능) 다음처럼 진행 1234567891011121314151617181920212223&quot;&quot;Do you accept the previously read EULA? accept/decline/quit: acceptInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81? ((y)es /(n)o/(q)uit) : nDo you want to install the OpenGL libraries? ((y)es /(n)o/(q)uit) : nDo you want to run nvidia-xconfig?This will update the system X configuration file so that the NVIDIA X driveris used. The pre-existing X configuration file will be backed up.This option should not be used on systems that require a customX configuration, such as systems with multiple GPU vendors.((y)es /(n)o/(q)uit) : nInstall the CUDA 9.0 Toolkit? ((y)es /(n)o/(q)uit) : yEnter Toolkit Location[ default is /usr/local/cuda-9.0 ] : EnterDo you want to install a symbolic link at /usr/local/cuda? ((y)es /(n)o/(q)uit) : yInstall the CUDA 9.0 Samples?(y)es/(n)o/(q)uit: n 다음으로 cuDNN 설치로 진행한다. cuDNN은 Cuda Deep Neural Network 의 약자로 CUDA에서 인공신경망 구조를 생성할 수 있도록 지원하는 NVIDIA에서 제공하는 라이브러리입니다. NVIDIA의 cuDNN 다운로드 링크를 접속하여 간단한 멤버십 가입을 통해 다운로드를 진행합니다. 다운로드 URL : https://developer.nvidia.com/rdp/cudnn-download CUDA 9.0 버전의 제일 상단의 cuDNN v7.1.4 Library for Linux 을 선택하여 다운로드합니다. (tgz 파일) 다운로드 받은 파일 경로로 이동하여 압축을 해제합니다. 123cd Downloadstar xvf cudnn-9.0-linux-x64-v7.1.tgz 압축을 해제하면 cuda 폴더가 생성되는데, 내부 파일들을 다음과 같이 복사합니다. 123sudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+x /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 모든 과정이 끝나면 홈디렉토리로 이동하여 .bashrc 파일에 CUDA의 PATH를 등록합니다. 1234sudo vi ~/.bashrcexport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 내용을 저장하고 닫습니다. bashrc의 변경사항을 바로 반영하도록 다음 명령을 실행합니다. 1source ~/.bashrc 설치가 잘 완료되었는지 확인하기 위해 nvcc -V를 통해 CUDA 버전을 확인할 수 있습니다. 1nvcc -V 2.2 YOLO_mark를 이용한 데이터 labelingsouce : /home/kipang/workspace/Yolo_mark 기존의 coco dataset, 80개의 클래스의 객체인식외에 Custom으로 클래스 데이터셋을 생성하는 작업을 진행합니다. Yolo_mark는 custom 데이터를 생성하기 위한 marking 툴을 제공합니다.git명령을 통해 다음과 같이 github의 yolo-mark오픈소스를 git clone하여 빌드합니다. 1234git clone https://github.com/AlexeyAB/Yolo_mark.gitcd Yolo_markcmake .make 빌드가 끝나면 yolo_mark 바이너리 실행파일이 생성된 것을 확인한다. x64/Release/data/obj.data파일을 편집기로 열어서 학습될 객체의 갯수를 지정해준다. 1vi x64/Release/data/obj.data 12345classes=2 ## 클래스 갯수train = data/train.txt ## train 대상파일valid = data/valid.txt ## validation 파일names = data/obj.names ## 객체명backup = /path/to/bakcup ## weight 저장위치 train, valid는 각각 학습,검증에 사용될 이미지파일의 경로를 가지고 있어야한다. 12345678예시)vi train.txt-------------------/path/to/cat1.jpg/path/to/cat2.jpg/path/to/cat3.jpg/path/to/dog1.jpg.... obj.names 파일에는 객체명을 저장한다. 12345예시)vi obj.names-------------------catdog 위와 같이 .jpg 학습파일, train.txt 이미지경로, obj.names의 작업이 끝나면아래와 같이 argument를 주어 yolo-mark를 실행한다. 12yolo_mark &lt;JPG파일 경로&gt; &lt;train.txt 경로&gt; &lt;obj.names 경로&gt;예) yolo_mark /path/jpg/ /path/to/train.txt /path/to/obj.names 실행하게 되면 아래 그림처럼 labeling 작업 툴 화면이 나타나게 된다. 실행화면: Keymap: Key 동작 마우스 드래그 &amp; 드롭 box 그리기 숫자키 0~9, ←,→ 객체 id 지정 space 다음 /assets\\yolov3 c 모든 box 지우기 z 취소,undo ESC 종료 여러 객체가 있을때 숫자키, 좌우키로 객체 id를 변환하면서 지정할 수도 있다. Box를 지정할때 마다 jpg 폴더에 해당파일의 .txt가 생성되며 내용에는 1객체id &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt; 처럼 box 작업을 한 내역이 저장되어 있다. Custom 데이터생성은 Yolo_mark를 사용하거나 LabelImg : https://github.com/tzutalin/labelImg 으로도 작업할 수 있다. 2.2.1 Video frame 저장yolo_mark에서는 Video 파일의 특정 N frame마다 JPG Image로 저장하는 기능을 제공한다. 1./yolo_mark &lt;프레임 저장경로&gt; cap_video &lt;Video파일&gt; &lt;N&gt; N을 10으로 지정하면 10번째 프레임마다 지정한 경로에 JPG로 저장된다.저장된 프레임 데이터에 labeling 작업을 진행하여 학습데이터로 활용할 수 있다. 2.3 Custom 데이터셋 학습2.3.1 Preparationcustom 데이터셋으로 학습을 진행하기 위해 앞서 진행하던 Yolo-mark의 x64/Release/yolo-obj.cfg파일을 수정한다. 1vi x64/Release/yolo-obj.cfg 내용 하단의 244줄, 230줄 두 부분을 수정해야 한다. 1234567891011[convolutional]...##224줄filters=21...[region]...###230줄classes=2... filters에 convolutional레이어의 필터값을 넣는데 다음과 같은 식에 따라 입력해준다. 12Yolov2 : (클래스수 + 5) x 5 ex) 클래스: 2, filter : 35Yolov3 : (클래스수 + 5) x 3 classes에는 클래스 갯수를 입력해준다. 다음으로 darknet을 실행할 수 있는 경로로 이동하여 아래와 같이 학습을 진행한다. 2.3.2 Trainingconvolutional layer를 구성하기 위해 사전에 학습된 가중치 데이터를 다운로드 받는다. (76MB) 1wget http://pjreddie.com/media/files/darknet19_448.conv.23 다운로드가 끝나면 custom dataset 레이블링 작업에 사용된 obj.data,obj.names,train.txt, darknet19_448.conv.23 파일을 darknet 실행경로로 옮겨준다. 아래와 같이 darknet detector train명령을 통해 학습을 실행시킨다. 1darknet detector train data/obj.data cfg/yolo-obj.cfg darknet19_448.conv.23 -dont_show 학습을 진행하면 위 그림과 같이 학습단계가 log형식으로 터미널상에 뿌려지는데왼쪽부터가 학습 iteration 횟수, avg loss 가 error rate 이다. Yolo document 에 따르면 클래스당 적어도 2000 번의 iteration을 수행해야한다고 나타나있다.즉, 클래스가 3종류이면 적어도 3 x 2000=6000번 정도의 iteration을 진행해야 한다. 또한 avg loss는 0.xxxxx 이하로 나타날때 Overfitting이 발생할 수 있으므로 해당 수치정도까지 학습을 진행하도록 권장되어있다. 학습은 100번의 iteration 마다 obj.data에 명시해둔 backup 경로에 checkpoint 형식으로 저장된다. 학습도중 예기치 못하게 종료되었을때 backup 된 weight로 중단된 지점부터 다시 학습을 진행할 수 있다. 1darknet detector train data/obj.data cfg/yolo-obj.cfg &lt;weight파일&gt; -dont_show 학습을 마쳤다면 테스트를 아래와 같이 명령어로 수행해 볼 수 있다. 1darknet detector test data/obj.data cfg/yolo-obj.cfg &lt;weight파일&gt; &lt;Image 파일&gt; 2.4 번호판 인식 테스트2013장의 레이블링된 번호판 데이터를 45000번 학습된 Custom weight 데이터를 이용하여 블랙박스 영상에서 번호판 객체만을 인식시켜보는 테스트를 진행한다. 학습에 사용된 데이터는 1675장의 정지된 차량이미지 + 338장의 동영상 프레임 이미지이다. 다음으로는 각 Neural Network 프레임워크 별로 영상/이미지 객체 인식을 수행해본다. weight 위치 :/data/plate/yolo-cits_final.weights 2.4.1 pjreddie/darknetGithub 링크 source : /home/kipang/workspace/pjreddie-darknet .data, .cfg 위치 : /data/plate/cits.data, /data/plate/yolo-cits.cfg darknet은 Yolo구현에 필요한 Neural Network 프레임워크이다. C++, CUDA로 작성되었으며 Github에 오픈소스로 공개되어 있다. darknet 소스를 내려받으면 Makefile을 수정하여 GPU 호환을 활성화시킨다. 1234567891011git clone https://github.com/pjreddie/darknetcd darknetvi Makefile------------GPU=1CUDNN=1OPENCV=1OPENMP=0....------------make -j$(nproc) 빌드가 끝나면 darknet 실행 바이너리 파일이 생성된다. Custom weight 데이터로 객체인식을 수행하기 위해선 아래와 같이 실행해준다. 1darknet detector demo &lt;.data파일&gt; &lt;.cfg파일&gt; &lt;weight파일&gt; &lt;media파일&gt; -thresh &lt;임계값&gt; -thresh &lt;임계값&gt;은 프레임에 클래스가 인식되었을때 해당 임계값을 초과할때만 객체로 인식하기위한 옵션이다. (default = 0.25, 25%)따라서, 임계값을 조절하여 객체인식률을 조절할 수 있다. 1234## For Imagedarknet detector test data/cits.data cfg/yolo-cits.cfg weight/yolo-cits.weight ~/Video/carpark.jpg## For Video streamdarknet detector demo data/cits.data cfg/yolo-cits.cfg weight/yolo-cits.weight ~/Video/carpark.mp4 실행 시 Video Player가 동작하여 결과를 Display 해준다. 2.4.2 alexyAB/darknetsource : /home/kipang/workspace/alexyAB-darknet 2.4.1 pjreddie/darknet 프로젝트를 fork하여 개발된 프레임워크로 몇가지 추가기능과 성능개선이 포함되어있다. Github 링크 상에 오픈소스로 공개되어있다. 빌드 순서 123456789101112cd darknetvi Makefile--------------GPU=1CUDNN=1OPENCV=1...--------------mkdir buildcd buildcmake ..make -j$(nproc) 추가기능은 -thresh 처럼 flag를 실행문 끝에 추가하여 아래 동작들을 수행할 수 있다. 1234567891011## Player 화면 숨기기darknet detector demo &lt;.data파일&gt; &lt;.cfg파일&gt; &lt;weight파일&gt; &lt;Media파일&gt; -dont_show## 파일로 저장하기darknet detector demo &lt;.data파일&gt; &lt;.cfg파일&gt; &lt;weight파일&gt; &lt;Media파일&gt; -out_filename &lt;파일명&gt;## Box 좌표 표시darknet detector demo &lt;.data파일&gt; &lt;.cfg파일&gt; &lt;weight파일&gt; &lt;Media파일&gt; -ext_output## Web 브라우저 결과 출력 (http://xxx:port 접속)darknet detector demo &lt;.data파일&gt; &lt;.cfg파일&gt; &lt;weight파일&gt; &lt;Media파일&gt; -http_port &lt;port&gt; 명령어를 실행하게 되면 Frame per Second로 와 같이 나타난다. 2.4.3 DeepVisionX PlayerGithub 링크 source : /home/kipang/workspace/deep-vision-x-player 기존 darknet 기반의 프레임워크로 stream 객체 인식시 동작하는 Video Player가 변경되었으며 실행시 cfg와 data파일의 flag 입력 순서가 다르다. 빌드순서 1234567891011cd deep-vision-x-playervi CMakeLists.txt------------------------option(USE_OPENCV \"Use OpenCV\" ON)option(USE_CUDA \"Use CUDA\" ON)option(USE_RELEASE_MODE \"Use CUDA\" ON)------------------------mkdir buildcd buildcmake ..make -j$(nproc) 1build/bin/DeepVisionXPlayer &lt;.cfg파일&gt; &lt;.data파일&gt; &lt;weight파일&gt; &lt;Media파일&gt; 동영상의 프레임별 객체를 스캔/탐지하는데 걸린 시간, 인식된 객체에 대한 confidence 수치를 나타나게 된다.","link":"/2018/09/11/Yolov3/"},{"title":"Apache Zeppelin 설치","text":"Apache Zeppelin 공홈 다운로드 &amp; 설치wget 명령어로 아파치 미러사이트에서 다운로드 받습니다. 1wget http://apache.mirror.cdnetworks.com/zeppelin/zeppelin-0.8.2/zeppelin-0.8.2-bin-all.tgz tar 로 압축을 해제 및 적당한 위치로 옮겨 줍시다. 123456tar xvf zeppelin-0.8.2-bin-all.tgzmv zeppelin-0.8.2-bin-all /opt/zeppelin-0.8.2## 유저 권한 부여chown -R hdfs:hadoop /opt/zeppelin Zeppelin의 bin 파일들을 등록시킵니다. /etc/profile.d 디렉토리에 적당한 이름의 스크립트 파일 zeppelin.sh 을 생성하고 아래 내용들을 입력해줍니다. 123export \"ZEPPELIN_HOME=/opt/zeppelin-0.8.2\" &gt;&gt; /etc/profile.d/zeppelin.sh # zeppelin 홈 디렉토리 export \"HADOOP_CONF_DIR=/opt/hadoop-2.7.7/etc/hadoop\" &gt;&gt; /etc/profile.d/zeppelin.sh # hadoop 설정 파일export \"PATH=$PATH:$ZEPPELIN_HOME/bin\" &gt;&gt; /etc/profile.d/zeppelin.sh 추가로 환경변수설정들을 $ZEPPELIN_HOME/conf/zeppelin-env.sh에 넣어줍니다. 필자는 포트번호와 url 설정, spark 경로들을 설정했습니다. 주요 설정 입력123echo \"export ZEPPELIN_ADDR=centos-namenode-1\" &gt;&gt; $ZEPPELIN_HOME/conf/zeppelin-env.shecho \"export ZEPPELIN_PROT=8889\" &gt;&gt; $ZEPPELIN_HOME/conf/zeppelin-env.shecho \"export SPARK_HOME=/opt/spark-2.4.4\" &gt;&gt; $ZEPPELIN_HOME/conf/zeppelin-env.sh 유저 생성zeppelin은 유저, 권한, 암호화 등의 관리를 Apache Shiro로 채택했습니다. 아래처럼 shiro.ini 파일을 수정하여 유저들을 등록시킬 수 있습니다. 1cp $ZEPPELIN_HOME/conf/shiro.ini.template $ZEPPELIN_HOME/conf/shiro.ini 12# shiro 수정vim $ZEPPELIN_HOME/conf/shiro.ini &lt;user&gt; = &lt;id&gt;, &lt;pw&gt; 순으로 입력해줍니다. 123[users]admin = admin, adminhdfs = hdfs, admin 설정이 끝났으면 재로그인하여 아래처럼 입력하여 zeppelin 서버를 데몬으로 실행시킬 수 있습니다. 실행1zeppelin-daemon.sh start 브라우저를 통해 url:port 로 접속하면 zeppelin 화면을 볼 수 있습니다. 필자의 경우 port번호를 8889로 설정했습니다. 1http://[url].8889 unsplash-logoArnold Francisca","link":"/2019/11/10/Zeppelin/"},{"title":"Proxmox VM에 Windows 10 설치하기","text":"Proxmox에서 VM을 생성하고, Windows 10 운영체제를 설치하는 방법에 대해 소개하겠습니다. Windows, VirtIO 이미지 업로드먼저 Windows 10 ISO 이미지를 Proxmox ISO image에 등록합니다. 등록방법은 기존 리눅스 ISO 이미지 업로드와 동일하게 진행하면 됩니다. Server &gt; local &gt; Content &gt; Upload Select File로 로컬 파일시스템에서 이미지를 찾아서 업로드 시켜줍니다. 추가로 KVM에서 Windows에서의 장치 드라이버를 지원하기위해선 VirtIO 라는 드라이버를 추가해야합니다. 관련정보 링크 로 접속해서 stable 버전의 VirtIO를 다운로드합니다. iso 확장자 이미지를 받습니다. Windows 10 ISO 업로드와 동일하게 이미지를 업로드 시킵시다. VM 생성이미지 업로드가 끝났으면 VM을 생성해줍니다. 먼저 생성할 VM에 대한 기본정보를 입력합니다. VM ID 와 Name을 입력하고 넘어갑시다. OS 선택화면 입니다. 앞서 업로드시킨 Window 10 이미지를 선택해줍니다. Guest OS 에 Type을 Microsoft Windows로 설정해줍니다. 그래픽카드 및 컨트롤러 설정입니다. 특별히 설정할게 없으면 넘어갑시다. 디스크 공간 설정입니다. 적절한 공간을 설정해줍시다. 윈도우는 최소 32GB의 저장공간이 필요하므로 적당히 넣어주면 됩니다. 다음으로 CPU 설정화면입니다. 적당한 코어 수를 입력해주면 됩니다. 다음으로 Memory 설정화면입니다. 저는 최소 1GB, 최대 4GB로 설정해줬습니다. 마지막으로 Network 설정화면입니다. Model에서 여러가지 가상 네트워크 인터페이스를 지원하는데 저는 리얼택으로 선택했습니다. 설정한 값들을 정리한 표입니다. 왼쪽하단에 Start After Boot를 선택해주면 설정완료 후 바로 VM이 시작하게 됩니다. 드라이버 설정VM &gt; Console 창에서 곧바로 NoVNC를 통해 설치화면을 볼 수 있습니다. 키보드 설정 &gt; 정품 키 인증 &gt; 운영체제 선택 등은 생략하겠습니다. Windows 설치 위치 화면에 마운트된 디스크가 조회되지 않는데 VirtIO ISO 이미지를 넣고 드라이버를 잡아주면 설정화면에 나타나게 됩니다. 잠시 빠져나와서 VM &gt; Hardware &gt; ADD &gt; CD/DVD Drive로 접속합니다. 그리고 앞서 업로드 시킨 VirtIO 이미지를 등록합니다. ISO 이미지를 추가하면 빨간 글씨로 보이는데 VM 을 완전히 재부팅 시키면 검정글씨로 바뀌면서 정상적으로 이미지가 CD로 마운트됩니다. VM을 오른쪽 클릭하거나 위쪽 상단의 Stop 버튼으로 완전히 종료시킨후 다시 부팅시킵시다. 정상적으로 종료되지 않을땐 proxmox 터미널에서 qm list으로 ID 조회한 후 qm stop &lt;ID&gt; 명령으로 종료시킬 수 있습니다. 다시 VM을 구동시킨 후 Windows 설치 위치 화면으로 다시 돌아갑시다. 왼쪽 아래의 드라이버 로드를 클릭합니다. 이 컴퓨터의 하드웨어와 호환되지 않는 드라이버 숨기기 를 체크해주시고 찾아보기 &gt; VirtIO &gt; vioscsi &gt; w8.1 &gt; amd64 를 선택하여 다음 을 클릭하여 디스크 드라이버를 설치해줍시다. 다음과 같이 드라이브가 잡힌 것을 볼 수 있습니다. 다시 다음을 눌러서 설치를 마무리해주시면 됩니다. 추가적인 드라이버 문제는 VirtIO에서 해당 드라이버를 찾아서 설치 해주시면 됩니다. unsplash-logoThomas Jensen","link":"/2020/04/10/proxmox-windows/"},{"title":"Proxmox VM에 GNS3 서버 구축하기","text":"Proxmox에서 GNS3 server를 VM으로 구축하는 방법에 대해 알아보겠습니다. GNS3 VM 다운로드 및 압축해제https://www.gns3.com/software/download-vm 에 접속해서 로그인한후, Vmware Workstation을 다운로드 받습니다. 다운로드한 ZIP파일을 압축해제하면 GNS3 VM.ova 파일이 있습니다. 필자는 맥을 사용하므로 터미널에서 unzip으로 압축해제 했습니다. 123unzip GNS3.VM.VMware.Workstation.2.2.7.zip# Archive: GNS3.VM.VMware.Workstation.2.2.7.zip# inflating: GNS3 VM.ova 디렉토리를 하나 생성해주고 (gns3), ova 파일을 해당 디렉토리로 옮겨넣습니다. 12mkdir gns3mv GNS3 VM.ova gns3 gns3 디렉토리로 이동하여 tar로 압축을 다시 해제시킵니다. 12345tar xvf GNS3 VM.ova# x GNS3 VM.ovf# x GNS3 VM.mf# x GNS3_VM-disk1.vmdk# x GNS3_VM-disk2.vmdk 생성된 파일중에서 .vmdk 파일들을 Proxmox 서버로 옮깁니다. 1scp *.vmdk root@&lt;proxmox&gt;:/root/ Proxmox에서 VM 생성다운받은 .vmdk 파일들로 디스크를 VM에 마운트 시킨후 마운트된 디스크로 부팅시키면 설치가 완료됩니다. proxmox 웹 클라언트로 접속해서 VM을 생성합니다. 부팅 디스크는 1GB 이하로 잡습니다. (나중에 지움) proxmox 서버의 터미널로 접속해서 다음과 같이 명령어를 입력해줍시다. 12# qm importdisk &lt;VM ID&gt; GNS3_VM-disk1.vmdk &lt;디스크&gt;qm importdisk 106 GNS3_VM-disk1.vmdk SSD-2 똑같이 GNS3_VM-disk2.vmdk파일도 동일하게 실행합니다. VM 콘솔로 이동해서 Hardware설정에 들어간후 부팅디스크(1G)를 Detach한 후 Remove 로 제거합니다. 그후 추가한 디스크들을 클릭해서 Bus/Device를 IDE로 변경해줍니다. Option 설정으로 들어가서 Boot Order를 ide0으로 변경해줍니다. 이제 VM을 구동시킵니다. 정상적으로 구동된 모습을 볼 수 있습니다. WEB-UI브라우저로 WEB-UI에 접근할 수 있습니다. 1http://&lt;ip&gt;:3080 unsplash-logoJordan Harrison","link":"/2020/02/12/proxomox-gns3/"},{"title":"NiFi 와 TensorflowOnSpark를 활용한 이미지 프로세싱","text":"전 회사에서 다뤘던 Apache NiFi 와 TensorflowOnSpark를 활용해서 이미지 프로세싱까지 해보는 내용을 다루겠습니다. Apache NiFi Apache NiFi supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic. Apache NiFi DataFlow, ETL을 구성하는데 필요하는데 필요한 데이터 트랜스포메이션, 전송, flow 로직등을 제공하는 오픈소스 프로젝트입니다. NiFi는 다음과 같은 특징을 가지고 있습니다. 직관적인 웹 인터페이스 제공 모든 데이터 흐름 로직에 대한 디자인, 조절, 모니터링 등을 브라우저를 통해서 조작할 수 있다. 데이터 flow 데이터의 source 에서 target까지의 흐름을 제어할 수 있습니다. 다양한 source 지원 HDFS, S3, HBase 등의 다양한 데이터 저장소와 데이터를 불러올 수 있는 API를 제공하며, processor를 통해 다양한 로직을 개발할 수 있습니다. 보안 SSL, SSH, HTTP, 데이터 암호화 등의 보안기능 제공 홈페이지에 기제된 특징대로 다양한 데이터 원천(source)로 부터 데이터 트랜스포메이션을 제공하며 특정 이벤트를 추가하여 다양한 용도로 활용가능합니다. NiFi를 활용하여 HDFS로부터 이미지 추출부터 TensorflowOnSpark를 활용한 이미지 프로세싱에 대해서 다뤄보겠습니다. NiFi는 Niagara File을 줄임말입니다. NiFi 설치Centos7 에서 설치를 진행합니다. NiFi 다운로드 페이지에서 NiFI Binaries 버전을 확인합니다. 저는 1.10.0 버전을 기준으로 진행하겠습니다. centos에서 적당한 위치로 이동한 후 wget 명령으로 다운로드 받겠습니다. 12# cd /some/where/nice/wget https://archive.apache.org/dist/nifi/1.10.0/nifi-1.10.0-bin.tar.gz tar xvf 명령으로 압축을 해제해줍시다. 1tar xvf nifi-1.10.0-bin.tar.gz JVM 설정NiFi는 JVM 기반으로 동작하므로 JAVA_HOME 경로를 ~/.bashrc에 넣어줍니다. 123echo \"export JAVA_HOME=/where/to/java\" &gt;&gt; ~/.bashrcsource ~/.bashrc 편의성을 위해 $NIFI_HOME 과 바이너리 파일들을 PATH에 추가합시다. 12echo \"export NIFI_HOME=/where/to/nifi\" &gt;&gt; ~/.bashrcecho \"export PATH=$PATH:$NIFI_HOME/bin\" &gt;&gt; ~/.bashrc NiFi 설정NiFi를 구동하면 웹 브라우저에서 해당 서버의 IP와 특정 포트를 통해 접근할 수 있습니다. 서버 URL과 포트등의 설정을 변경하고자 하면 $NIFI_HOME/conf/nifi.properties를 수정합니다. 1vi $NIFI_HOME/conf/nifi.properties 아래에 보면 `# web properties #`에서 포트와 호스트주소를 수정할 수 있습니다. 12nifi.web.http.host=nifi.web.http.port=8080 JVM 메모리 설정은 $NIFI_HOME/conf/bootstrap.conf에서 java.arg.2=-Xms부분을 수정하여 조절할 수 있습니다. NiFi 실행NiFi 실행 및 종료는 $NIFI_HOME/bin/nifi.sh 스크립트로 관리합니다. 12$NIFI_HOME/bin/nifi.sh# Usage nifi {start|stop|run|restart|status|dump|diagnostics|install|stateless} 필자는 해당 bin 디렉토리를 PATH에 등록했으므로 쉘에서 nifi.sh start로 nifi 서버를 실행하겠습니다. 12345$ nifi.sh start# Java home: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64# NiFi home: /opt/nifi# Bootstrap Config File: /opt/nifi/conf/bootstrap.conf ps -ef | grep nifi로 NiFi 데몬을 확인하실 수 있습니다. NiFi 로그는 $NIFI_HOME/logs에 저장됩니다. 설정파일을 수정하여 경로를 수정할 수 있습니다. 이제 브라우저를 통해서 NiFi에 접속합니다. 1http://&lt;ip&gt;:&lt;port&gt;/nifi IP와 포트를 포함한 URL을 입력후 /nifi를 추가하여 NiFi에 접속합니다. unsplash-logoVince Veras","link":"/2019/01/23/nifi-tensorflowonspark/"},{"title":"Proxmox VM에 EVE-NG 구축하기","text":"Proxmox에서 EVE-NG 서버를 구축하는 방법에 대해서 알아보겠습니다. ISO 다운로드EVE-NG 홈페이지에서 Free EVE Community Edition의 ISO 이미지를 다운로드 받습니다. ISO 업로드Proxmox 웹 인터페이스로 이동해서 ISO 이미지를 업로드합니다. Nested VirtualizationVM 내부에서도 가상화를 지원하도록 설정합니다. 먼저 Proxmox에서 접속해서 내부 가상화 활성화 여부를 확인합니다. 123# Proxmoxcat /sys/module/kvm_intel/parameters/nested# N N이면 비활성 상태로 다음과 같이 설정해서 활성화 시켜줍시다. 123456789# Intel 일경우echo \"options kvm-intel nested=Y\" &gt; /etc/modprobe.d/kvm-intel.conf# AMD 일경우echo \"options kvm-amd nested=1\" &gt; /etc/modprobe.d/kvm-amd.conf# 커널 모듈 reload, AMD 일경우 kvm-amd로modprobe -r kvm_intelmodprobe kvm_intel 설정후 시스템을 재부팅합니다. VM 생성업로드한 ISO 이미지로 VM을 생성합시다. 설치가 끝나면 VM을 구동시켜 Console 화면에서 설치를 진행합니다. EVE-NG 설치VM 콘솔화면에서 설치를 진행합니다. 설치과정은 Ubuntu Server와 유사합니다. 설치과정은 SKIP!! EVE-NG 설정로그인 화면입니다. ID는 root, PW는 eve로 입력합니다. 비밀번호를 재입력해줍시다. hostname설정입니다. DNS 설정입니다. 다음으로 서버 네트워크 설정입니다. 필자는 dhcp서버가 있기에 dhcp로 설정했습니다. NTP 서버 설정입니다. 접속해당 IP로 브라우저에서 접속하면 서버화면을 조회할 수 있습니다. unsplash-logoThomas Jensen","link":"/2020/02/13/proxmox-eveng/"},{"title":"람다 아키텍쳐","text":"Lambda Architecture, 람다 아키텍쳐데이터 처리 방법은 다양하고 넓게 분류해보면 배치, 실시간 으로 나뉜다. 원하는 방식을 사용하여 원하는 결과를 얻을 수 있지만 어떤 상황에서는 두 처리 방법의 데이터가 모두 필요한 경우가 있다. 이때 데이터 병합 문제가 발생할 수 있는데 Lambda Architecture, 람다 아키텍쳐 를 적용하면 문제를 해결할 수 있다. 람다 아키텍쳐는 높은 확장성과 분산 컴퓨팅 성능을 제공하며, 배치와 실시간 처리를 통해 결과적으로 일관성 있는 데이터를 제공한다. Data SourceData Source데이터 획득 계층(Data Acquisition Layer)데이터 획득 계층…메시지 전달 계층(Messaging Layer)메시지 전달 계층…데이터 흡수 계층(Data Ingestion Layer)데이터 흡수 계층…람다 계층람다 계층배치 계층(Batch Layer)배치 계층…속도 계층(Speed Layer)속도 계층…제공 계층(Serving Layer)제공 계층…데이터 저장 계층(Data Storage Layer)데이터 저장 계층…Viewer does not support full SVG 1.1 데이터 흡수 계층, Data Ingestion Layer 전달되는 데이터의 속도를 제어할 수 있어야한다. 변화하는 작업 부하에 따라 확장이 가능한 유연성 Fault-tolerant 및 fail-over 지원 멀티 스레드 및 멀티 이벤트 처리 가능 데이터를 요청하는 계층에 맞게 빠르게 변환가능 데이터는 purest form (순수한) 형태를 유지해야한다. 데이터 흡수 계층은 메시지 전달 계층의 메시지를 소비하고 람다 계층으로 흡수하는데 필요한 변환을 수행해 다음 계층의 저장 및 처릴르 쉽게 만들어 준다. 이 계층은 일관된 방식으로 메시지를 소비하는 것을 보장해야 하며, 이를 통해 모든 메시지가 유실되지 않고 최소 한 번은 처리되게 해야 한다. 람다 계층일반적으로 배치 계층과 속도 계층으로도 알려진 실시간 계층을 포함한 두개의 계층으로 구성된다. 배치 계층, Batch Layer 흡수된 데이터의 처리 대규모 데이터를 처리하는 가장 전통적인 방법이며, 대게 오랜 시간에 걸쳐 수행된다. 원본 데이터를 모델링된 데이터로 변환하는 것이 주 목적 시스템 자원을 최대한 활용해 흡수된 데이터를 처리함 장시간 연산이 요구되는 데이터에 대해 높은 품질의 데이터를 제공 Falut-tolerant 지원 흡수된 원본 데이터에 대한 모델링 데이터를 생성을 하기위해 머신 러닝과 데이터 분석 처리를 지원 de-duplication, 중복 제거등을 통해 데이터 전체적으로 품질 향상을 제공 Hadoop은 전통적인 배치 처리 방식보다 더 효율적이고 확장성있는 배치 처리가 가능한 프레임워크와 기술을 모두 제공한다. 하둡 기반의 배치 처리는 데이터와 처리의 분산을 하부에 있는 하둡 프레임워크가 담당하게 함으로써 map/reduce 작어비 특정 데이터 처리에만 집중할 수 있게 하므로 전통적인 방식으로 구현된 배치 처리에 비해 엄청난 성능 향상을 제공한다. Map/Reduce 패러다임은 메인프레임의 등장 이후 많은 어플리케이션에서 활용해 왔기 때문에 새로운 개념이 아니다. 배치를 여러 과정으로 나눈 후 단일 출력을 생성하기 위해서 모든 출력물을 하나로 모으는 것이다. 속도 계층, Speed Layer 근실시간 데이터 칠 람다 아키텍처의 실시간 처리 계층이다. 데이터는 흡수되는 즉시 처리되며, 처리된 데이터는 저장 계층에 저장된다. 흡수된 데이터에 대한 실시간으로 처리하는 작업을 수행 높은 동시성을 지원 및 데이터 처리 기능, 신속하고 효율적으로 수행되야함 실시간 데이터 모델을 생성할 수 있어야 한다. 처리 대상이 쌓이지 않도록 저장 계층에 빠르게 접근할 수 있어야 한다. 흡수 계층으로 부터 분리 되어야 한다. 요건에 맞는 데이터를 제공하기 위해 배치 처리도니 데이터 세트와 병합할 수 있는 모델을 생성할 수 있어야 한다. 초기에는 스트리밍 기술로 Flume과 HDFS를 사용했으며, 실시간 처리에 하둡 배치 처리에 의존하는 것이 적합하지 않다는 것은 빠르게 증명되어, 이에 특화된 개별 프레임워크가 만들어졌다.(Spark) 초기에는 개별 프레임워크들은 Hadoop 에코들과 잘 연동되지 않았지만 기술의 발전하면서 운영과 관리의 단순화를 위해 하둡과 통합되었다. 데이터 저장 계층, Data Storage Layer 모든 데이터 저장 람다 아키텍쳐에서 흡수된 데이터에 대한 처리는 두가지(배치, 속도)가 있고, 각 처리에마다 요구되는 데이터는 매우 다르다. 예를 들어 배치 처리의 경우 HDFS 저장계층에 적합한 순차적 읽기와 쓰기 연산이 일어난다. 하지만 실시간 처리의 경우 빠른 조회와 빠른 쓰기가 필요하므로 HDFS는 적합하지 않다. 실시간 처리를 위해선 indexed data(색인) 유형의 데이터 계층이 필요하다. 순차(serial) 연산과 임의(random) 연산을 모두 지원해야 한다. 데이터 솔루션에 적합한 활용 패턴에 따라 계층화되야 한다. 배치뿐만 아니라 실시간 처리를 위한 대용량 데이터를 제어할 수 있어야 한다. 다양한 구조를 갖는 데이터 저장소를 지원하기 위해 유연성과 확장성을 제공해야 한다. 제공 계층, Serving Layer 데이터 제공 및 export 데이터를 소비하는 application으로 최종 형태의 데이터를 delivery한다. 서비스를 통해 데이터를 제공하는 것이 일반적. 이런 서비스를 데이터 서비스라고 함. 데이터 전송을 위해 다양한 프로토콜을 지원할 수 있다. 데이터가 외부로 나가는 방법은 요청에 의한 응답과 규약에 따른 일반적 전송으로 나뉠수 있다. 데이터를 소비하는 application으로 제공할 수 있는 다양한 메커니즘을 제공 데이터를 소비하는 application과 맺은 규약을 준수해야한다. 배치, 실시간으로 처리된 데이터에 대한 view를 지원 확장성이 있어야 하며, 소비 대상 application에 빠르게 대응해야 한다. 데이터 획득 계층, Data acquisition Layer 데이터 원천에서 데이터 획득 관계형 데이터베이스 XML/JSON 데이터, 시스템간에 오가는 메시지등이 정형 데이터, 이메일, 채팅, 문서 등의 반정형 데이터를 다룬다. 획득 계층의 핵심은 데이터 레이크에서 처리할 수 있는 형태의 메시지로 데이터를 변환하는 것이다. 따라서 다양하게 정의된 스키마 정의를 수용할 수 있는 유연성을 가져야 하고, 동시에 전체 메시지를 끊임없이 데이터 레이크로 밀어 넣을 수 있는 빠른 접속 메커니즘을 갖고 있어야 한다. Falut-tolerant 메시지 저장용 로컬 버퍼를 지원해야함 메시지 전달 계층 데이터 전송 보장 데이터 레이크 아키텍처에서 메시지 지향 미들웨어를 형성해 메시지의 전달을 보장하면서 여러 계층을 분리하는 핵심 계층이다. 메시지 전달을 보장하려면 메시지가 저장되어야 한다. 메시지 저장은 일반적으로 내부디스크에서 이루어진다. 일반적으로 HDD로도 충분하지만, 초당 수백만 메시지를 처리해야하면 SSD를 사용하는 것이 I/O에서 유리하다. 또 다른 기능으로는 Queue 적제, Queue 인출 능력이다. 대부분의 메시징 프레임워크는 메시지의 발행과 소비를 관리하기 위한 적재 및 인출 메커니즘을 제공한다. 모든 메시징 프레임워크는 내부 자원에 접속할 수 있는 자체 라이브러리를 제공한다. 모든 메시지 지향 미들웨어는 일반적으로 Queue와 Topic 이라는 메시징 구조를 이용해 두가지 유형의 통신을 지원한다. Queue는 하나의 Consumer가 모든 메시지를 단 한번만 소비하게 하는 point-to-point 통신에 사용된다. Topic은 메시지가 한 번 발행된 후 consumer에 의해 여러번 소비 가능한 발행/구독 메커니즘을 제공하는데 사용된다. 따라서 메시지는 컨슈머마다 한 번씩 여러번 소비될 수 있다. Photo by Samuel Sianipar on Unsplash","link":"/2020/06/20/Lambda_Arch/"}],"tags":[{"name":"Hive","slug":"Hive","link":"/tags/Hive/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"CUDA","slug":"CUDA","link":"/tags/CUDA/"},{"name":"cuDNN","slug":"cuDNN","link":"/tags/cuDNN/"},{"name":"Tensorflow","slug":"Tensorflow","link":"/tags/Tensorflow/"},{"name":"Windows","slug":"Windows","link":"/tags/Windows/"},{"name":"Jupyter","slug":"Jupyter","link":"/tags/Jupyter/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Oracle","slug":"Oracle","link":"/tags/Oracle/"},{"name":"Centos","slug":"Centos","link":"/tags/Centos/"},{"name":"Proxmox","slug":"Proxmox","link":"/tags/Proxmox/"},{"name":"KVM","slug":"KVM","link":"/tags/KVM/"},{"name":"Hypervisor","slug":"Hypervisor","link":"/tags/Hypervisor/"},{"name":"LXC","slug":"LXC","link":"/tags/LXC/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"NVIDIA","slug":"NVIDIA","link":"/tags/NVIDIA/"},{"name":"Zeppelin","slug":"Zeppelin","link":"/tags/Zeppelin/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","link":"/tags/Computer-Vision/"},{"name":"gns3","slug":"gns3","link":"/tags/gns3/"},{"name":"EVE-NG","slug":"EVE-NG","link":"/tags/EVE-NG/"},{"name":"NiFi","slug":"NiFi","link":"/tags/NiFi/"},{"name":"Lambda","slug":"Lambda","link":"/tags/Lambda/"}],"categories":[{"name":"Hadoop","slug":"Hadoop","link":"/categories/Hadoop/"},{"name":"CUDA","slug":"CUDA","link":"/categories/CUDA/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Oracle","slug":"Oracle","link":"/categories/Oracle/"},{"name":"Hypervisor","slug":"Hypervisor","link":"/categories/Hypervisor/"},{"name":"Spark","slug":"Spark","link":"/categories/Spark/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"NiFi","slug":"NiFi","link":"/categories/NiFi/"},{"name":"Data Lake","slug":"Data-Lake","link":"/categories/Data-Lake/"}]}